{
 "cells": [
  {
   "attachments": {
    "Unifor_logo.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Unifor_logo.png](attachment:Unifor_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINERA√á√ÉO DE TEXTOS E DA WEB\n",
    "\n",
    "## ATIVIDADE 01\n",
    "\n",
    "#### GRUPO 02:\n",
    "- Agenor J√∫nior\n",
    "- Nicole Wirtzbiki\n",
    "- Torricelli Evangelista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------\n",
    "INICIO ATIVIDADE 01 - Pr√©-processamento dos textos\n",
    "\n",
    "- Tokeniza√ß√£o\n",
    "- Lematiza√ß√£o\n",
    "- POS Tagging\n",
    "- Normaliza√ß√£o\n",
    "- Chunking\n",
    "- NER (entidades nomeadas)\n",
    "- Remo√ß√£o stop-words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANTO PACOTES NECESS√ÅRIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('average_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "language = \"english\"\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "import os\n",
    "#with redirect_stdout(open(os.devnull, \"w\")):\n",
    "    #nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Abertura do arquivo para tratamento dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>UNT</td>\n",
       "      <td>OFF</td>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IND</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OFF</td>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you‚Äôre drunk!!! @USER #MAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT</td>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>UNT</td>\n",
       "      <td>OFF</td>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT</td>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OTH</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OFF</td>\n",
       "      <td>97670</td>\n",
       "      <td>@USER Liberals are all Kookoo !!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>UNT</td>\n",
       "      <td>OFF</td>\n",
       "      <td>77444</td>\n",
       "      <td>@USER @USER Oh noes! Tough shit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GRP</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OFF</td>\n",
       "      <td>52415</td>\n",
       "      <td>@USER was literally just talking about this lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT</td>\n",
       "      <td>45157</td>\n",
       "      <td>@USER Buy more icecream!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IND</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OFF</td>\n",
       "      <td>13384</td>\n",
       "      <td>@USER Canada doesn‚Äôt need another CUCK! We alr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subtask_c subtask_b subtask_a     id  \\\n",
       "0       NaN       UNT       OFF  86426   \n",
       "1       IND       TIN       OFF  90194   \n",
       "2       NaN       NaN       NOT  16820   \n",
       "3       NaN       UNT       OFF  62688   \n",
       "4       NaN       NaN       NOT  43605   \n",
       "5       OTH       TIN       OFF  97670   \n",
       "6       NaN       UNT       OFF  77444   \n",
       "7       GRP       TIN       OFF  52415   \n",
       "8       NaN       NaN       NOT  45157   \n",
       "9       IND       TIN       OFF  13384   \n",
       "\n",
       "                                               tweet  \n",
       "0  @USER She should ask a few native Americans wh...  \n",
       "1  @USER @USER Go home you‚Äôre drunk!!! @USER #MAG...  \n",
       "2  Amazon is investigating Chinese employees who ...  \n",
       "3  @USER Someone should'veTaken\" this piece of sh...  \n",
       "4  @USER @USER Obama wanted liberals &amp; illega...  \n",
       "5                  @USER Liberals are all Kookoo !!!  \n",
       "6                   @USER @USER Oh noes! Tough shit.  \n",
       "7  @USER was literally just talking about this lo...  \n",
       "8                         @USER Buy more icecream!!!  \n",
       "9  @USER Canada doesn‚Äôt need another CUCK! We alr...  "
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#leitura para objeto dataframe\n",
    "tweets = pd.read_csv('/home/nico/√Årea de Trabalho/MineracaoDadosWeb/entrega 2/olid-training-v1.0.tsv', sep='\\t',encoding= 'utf-8')\n",
    "\n",
    "#convers√£o da coluna 'id' de inteiro para string\n",
    "tweets['id'] = tweets['id'].astype('str')\n",
    "\n",
    "#visualiza√ß√£o dos primeiros registros\n",
    "tweets = tweets[['subtask_c','subtask_b','subtask_a','id','tweet']]\n",
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1- Uso da fun√ß√£o \"print()\" para visualizar o arquivo na sua forma bruta.\n",
    "O arquivo corresponde a uma lista contendo 13240 strings. Cada string representa um tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tweets) # Imprime uma lista com 13240 strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Normaliza√ß√£o\n",
    "\n",
    "entrada: tweet  |  sa√≠da: tweet_normalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Importando o m√≥dulo \"REGEX\" para express√µes regulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_normalizado</th>\n",
       "      <th>tweet</th>\n",
       "      <th>id</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>she should ask a few native americans what th...</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>86426</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>go home youre drunk!      üëäüá∫üá∏üëä</td>\n",
       "      <td>@USER @USER Go home you‚Äôre drunk!!! @USER #MAG...</td>\n",
       "      <td>90194</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amazon is investigating chinese employees who ...</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>16820</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>someone shouldvetaken this piece of shit to a...</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>62688</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>obama wanted liberals &amp;amp; illegals to move...</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>43605</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   tweet_normalizado  \\\n",
       "0   she should ask a few native americans what th...   \n",
       "1                    go home youre drunk!      üëäüá∫üá∏üëä    \n",
       "2  amazon is investigating chinese employees who ...   \n",
       "3   someone shouldvetaken this piece of shit to a...   \n",
       "4    obama wanted liberals &amp; illegals to move...   \n",
       "\n",
       "                                               tweet     id subtask_a  \\\n",
       "0  @USER She should ask a few native Americans wh...  86426       OFF   \n",
       "1  @USER @USER Go home you‚Äôre drunk!!! @USER #MAG...  90194       OFF   \n",
       "2  Amazon is investigating Chinese employees who ...  16820       NOT   \n",
       "3  @USER Someone should'veTaken\" this piece of sh...  62688       OFF   \n",
       "4  @USER @USER Obama wanted liberals &amp; illega...  43605       NOT   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT       NaN  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "3       UNT       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalizacao_texto(tweet):\n",
    "    \n",
    "    #Normaliza√ß√£o de todas as palavras para caixa baixa\n",
    "    tweet = tweet.lower() \n",
    "    \n",
    "    #Pela import√¢ncia de manter pelo menos uma ocorr√™ncia de \"@user\" \n",
    "    #em cada tweet, foram removidas apenas suas repeti√ß√µes.\n",
    "    #tweet = re.sub('(@user |@user| @user )+',' @user ',tweet)\n",
    "    \n",
    "    #remove as men√ß√µes a usu√°rios de cada tweet\n",
    "    tweet = re.sub(r'@user', '', tweet, flags=re.MULTILINE)\n",
    "    \n",
    "    #Remo√ß√£o de repeti√ß√µes seguidas de acentua√ß√µes para n√£o \n",
    "    #perder o sentido durante a an√°lise do contexto dos tweets\n",
    "    tweet = re.sub('(!)+','!',tweet)\n",
    "    tweet = re.sub('(\")+','\"',tweet)\n",
    "    tweet = re.sub('(\\.)+','.',tweet)\n",
    "    \n",
    "    #Remo√ß√£o de todas as palavras que come√ßam com \"#\"\n",
    "    tweet = re.sub(r\"#(\\w+)\", ' ', tweet, flags=re.MULTILINE)\n",
    "      \n",
    "    #remove as palavras url\n",
    "    tweet = re.sub(r'url', '', tweet, flags=re.MULTILINE)\n",
    "    \n",
    "    #remove aspas e ap√≥stofres\n",
    "    tweet = re.sub('[\\'\"‚Äò‚Äô‚Äú‚Äù‚Ä¶]', '', tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "tweets['tweet_normalizado'] = tweets['tweet'].apply(normalizacao_texto)\n",
    "#tweets.head()  \n",
    "tweets[tweets.columns[::-1]].head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Tokeniza√ß√£o\n",
    "entrada: tweet_normalizado | sa√≠da: tweet_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer, sent_tokenize\n",
    "tweet_tokenizer = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_tokens</th>\n",
       "      <th>tweet_normalizado</th>\n",
       "      <th>tweet</th>\n",
       "      <th>id</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[she, should, ask, a, few, native, americans,...</td>\n",
       "      <td>she should ask a few native americans what th...</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>86426</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]</td>\n",
       "      <td>go home youre drunk!      üëäüá∫üá∏üëä</td>\n",
       "      <td>@USER @USER Go home you‚Äôre drunk!!! @USER #MAG...</td>\n",
       "      <td>90194</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[amazon, is, investigating, chinese, employee...</td>\n",
       "      <td>amazon is investigating chinese employees who ...</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>16820</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[someone, shouldvetaken, this, piece, of, shi...</td>\n",
       "      <td>someone shouldvetaken this piece of shit to a...</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>62688</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[obama, wanted, liberals, &amp;, illegals, to, mo...</td>\n",
       "      <td>obama wanted liberals &amp;amp; illegals to move...</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>43605</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        tweet_tokens  \\\n",
       "0  [[she, should, ask, a, few, native, americans,...   \n",
       "1        [[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]   \n",
       "2  [[amazon, is, investigating, chinese, employee...   \n",
       "3  [[someone, shouldvetaken, this, piece, of, shi...   \n",
       "4  [[obama, wanted, liberals, &, illegals, to, mo...   \n",
       "\n",
       "                                   tweet_normalizado  \\\n",
       "0   she should ask a few native americans what th...   \n",
       "1                    go home youre drunk!      üëäüá∫üá∏üëä    \n",
       "2  amazon is investigating chinese employees who ...   \n",
       "3   someone shouldvetaken this piece of shit to a...   \n",
       "4    obama wanted liberals &amp; illegals to move...   \n",
       "\n",
       "                                               tweet     id subtask_a  \\\n",
       "0  @USER She should ask a few native Americans wh...  86426       OFF   \n",
       "1  @USER @USER Go home you‚Äôre drunk!!! @USER #MAG...  90194       OFF   \n",
       "2  Amazon is investigating Chinese employees who ...  16820       NOT   \n",
       "3  @USER Someone should'veTaken\" this piece of sh...  62688       OFF   \n",
       "4  @USER @USER Obama wanted liberals &amp; illega...  43605       NOT   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT       NaN  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "3       UNT       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(tweet):\n",
    "    sents = sent_tokenize(tweet) #separando em senten√ßas\n",
    "    tokens = []\n",
    "    for word in sents:\n",
    "        tokens.append(tokenizer.tokenize(word)) #separando palavras\n",
    "    return tokens\n",
    "\n",
    "tweets['tweet_tokens'] = tweets['tweet_normalizado'].apply(tokenize)\n",
    "tweets[tweets.columns[::-1]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Remo√ß√£o de Stop Words \n",
    "entrada: tweet_tokens | sa√≠da: tweet_tokens_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_chunked</th>\n",
       "      <th>tweet_lemma</th>\n",
       "      <th>tweet_tagged</th>\n",
       "      <th>tweet_tokens_sw</th>\n",
       "      <th>tweet_tokens</th>\n",
       "      <th>tweet_normalizado</th>\n",
       "      <th>tweet</th>\n",
       "      <th>id</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[(ask, RB, O), (native, JJ, B-NP), (americans...</td>\n",
       "      <td>[[ask, native, american, take, .]]</td>\n",
       "      <td>[[(ask, RB), (native, JJ), (americans, NNS), (...</td>\n",
       "      <td>[[ask, native, americans, take, .]]</td>\n",
       "      <td>[[she, should, ask, a, few, native, americans,...</td>\n",
       "      <td>she should ask a few native americans what th...</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>86426</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[(go, VB, O), (home, NN, B-NP), (youre, NN, I...</td>\n",
       "      <td>[[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]</td>\n",
       "      <td>[[(go, VB), (home, NN), (youre, NN), (drunk, N...</td>\n",
       "      <td>[[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]</td>\n",
       "      <td>[[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]</td>\n",
       "      <td>go home youre drunk!      üëäüá∫üá∏üëä</td>\n",
       "      <td>@USER @USER Go home you‚Äôre drunk!!! @USER #MAG...</td>\n",
       "      <td>90194</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[(amazon, NN, B-NP), (investigating, VBG, O),...</td>\n",
       "      <td>[[amazon, investigate, chinese, employee, sell...</td>\n",
       "      <td>[[(amazon, NN), (investigating, VBG), (chinese...</td>\n",
       "      <td>[[amazon, investigating, chinese, employees, s...</td>\n",
       "      <td>[[amazon, is, investigating, chinese, employee...</td>\n",
       "      <td>amazon is investigating chinese employees who ...</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>16820</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[(someone, NN, B-NP), (shouldvetaken, VBD, O)...</td>\n",
       "      <td>[[someone, shouldvetaken, piece, shit, volcano...</td>\n",
       "      <td>[[(someone, NN), (shouldvetaken, VBD), (piece,...</td>\n",
       "      <td>[[someone, shouldvetaken, piece, shit, volcano...</td>\n",
       "      <td>[[someone, shouldvetaken, this, piece, of, shi...</td>\n",
       "      <td>someone shouldvetaken this piece of shit to a...</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>62688</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[(obama, RB, O), (wanted, VBD, O), (liberals,...</td>\n",
       "      <td>[[obama, want, liberal, &amp;, illegals, move, red...</td>\n",
       "      <td>[[(obama, RB), (wanted, VBD), (liberals, NNS),...</td>\n",
       "      <td>[[obama, wanted, liberals, &amp;, illegals, move, ...</td>\n",
       "      <td>[[obama, wanted, liberals, &amp;, illegals, to, mo...</td>\n",
       "      <td>obama wanted liberals &amp;amp; illegals to move...</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>43605</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       tweet_chunked  \\\n",
       "0  [[(ask, RB, O), (native, JJ, B-NP), (americans...   \n",
       "1  [[(go, VB, O), (home, NN, B-NP), (youre, NN, I...   \n",
       "2  [[(amazon, NN, B-NP), (investigating, VBG, O),...   \n",
       "3  [[(someone, NN, B-NP), (shouldvetaken, VBD, O)...   \n",
       "4  [[(obama, RB, O), (wanted, VBD, O), (liberals,...   \n",
       "\n",
       "                                         tweet_lemma  \\\n",
       "0                 [[ask, native, american, take, .]]   \n",
       "1        [[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]   \n",
       "2  [[amazon, investigate, chinese, employee, sell...   \n",
       "3  [[someone, shouldvetaken, piece, shit, volcano...   \n",
       "4  [[obama, want, liberal, &, illegals, move, red...   \n",
       "\n",
       "                                        tweet_tagged  \\\n",
       "0  [[(ask, RB), (native, JJ), (americans, NNS), (...   \n",
       "1  [[(go, VB), (home, NN), (youre, NN), (drunk, N...   \n",
       "2  [[(amazon, NN), (investigating, VBG), (chinese...   \n",
       "3  [[(someone, NN), (shouldvetaken, VBD), (piece,...   \n",
       "4  [[(obama, RB), (wanted, VBD), (liberals, NNS),...   \n",
       "\n",
       "                                     tweet_tokens_sw  \\\n",
       "0                [[ask, native, americans, take, .]]   \n",
       "1        [[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]   \n",
       "2  [[amazon, investigating, chinese, employees, s...   \n",
       "3  [[someone, shouldvetaken, piece, shit, volcano...   \n",
       "4  [[obama, wanted, liberals, &, illegals, move, ...   \n",
       "\n",
       "                                        tweet_tokens  \\\n",
       "0  [[she, should, ask, a, few, native, americans,...   \n",
       "1        [[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]   \n",
       "2  [[amazon, is, investigating, chinese, employee...   \n",
       "3  [[someone, shouldvetaken, this, piece, of, shi...   \n",
       "4  [[obama, wanted, liberals, &, illegals, to, mo...   \n",
       "\n",
       "                                   tweet_normalizado  \\\n",
       "0   she should ask a few native americans what th...   \n",
       "1                    go home youre drunk!      üëäüá∫üá∏üëä    \n",
       "2  amazon is investigating chinese employees who ...   \n",
       "3   someone shouldvetaken this piece of shit to a...   \n",
       "4    obama wanted liberals &amp; illegals to move...   \n",
       "\n",
       "                                               tweet     id subtask_a  \\\n",
       "0  @USER She should ask a few native Americans wh...  86426       OFF   \n",
       "1  @USER @USER Go home you‚Äôre drunk!!! @USER #MAG...  90194       OFF   \n",
       "2  Amazon is investigating Chinese employees who ...  16820       NOT   \n",
       "3  @USER Someone should'veTaken\" this piece of sh...  62688       OFF   \n",
       "4  @USER @USER Obama wanted liberals &amp; illega...  43605       NOT   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT       NaN  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "3       UNT       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stop_words(sents_list):\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    out_list = []\n",
    "    \n",
    "    for tokens_list in sents_list:\n",
    "        tokens = [x for x in tokens_list if not x in stop_words]\n",
    "        out_list.append(tokens)\n",
    "        \n",
    "    return out_list\n",
    "\n",
    "   \n",
    "tweets['tweet_tokens_sw'] = tweets['tweet_tokens'].apply(remove_stop_words)\n",
    "tweets[tweets.columns[::-1]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - POS Tagger\n",
    "\n",
    "entrada: tweet_tokens_sw | sa√≠da: tweet_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_tagged</th>\n",
       "      <th>tweet_tokens_sw</th>\n",
       "      <th>tweet_tokens</th>\n",
       "      <th>tweet_normalizado</th>\n",
       "      <th>tweet</th>\n",
       "      <th>id</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[(ask, RB), (native, JJ), (americans, NNS), (...</td>\n",
       "      <td>[[ask, native, americans, take, .]]</td>\n",
       "      <td>[[she, should, ask, a, few, native, americans,...</td>\n",
       "      <td>she should ask a few native americans what th...</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>86426</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[(go, VB), (home, NN), (youre, NN), (drunk, N...</td>\n",
       "      <td>[[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]</td>\n",
       "      <td>[[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]</td>\n",
       "      <td>go home youre drunk!      üëäüá∫üá∏üëä</td>\n",
       "      <td>@USER @USER Go home you‚Äôre drunk!!! @USER #MAG...</td>\n",
       "      <td>90194</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[(amazon, NN), (investigating, VBG), (chinese...</td>\n",
       "      <td>[[amazon, investigating, chinese, employees, s...</td>\n",
       "      <td>[[amazon, is, investigating, chinese, employee...</td>\n",
       "      <td>amazon is investigating chinese employees who ...</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>16820</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[(someone, NN), (shouldvetaken, VBD), (piece,...</td>\n",
       "      <td>[[someone, shouldvetaken, piece, shit, volcano...</td>\n",
       "      <td>[[someone, shouldvetaken, this, piece, of, shi...</td>\n",
       "      <td>someone shouldvetaken this piece of shit to a...</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>62688</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[(obama, RB), (wanted, VBD), (liberals, NNS),...</td>\n",
       "      <td>[[obama, wanted, liberals, &amp;, illegals, move, ...</td>\n",
       "      <td>[[obama, wanted, liberals, &amp;, illegals, to, mo...</td>\n",
       "      <td>obama wanted liberals &amp;amp; illegals to move...</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>43605</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        tweet_tagged  \\\n",
       "0  [[(ask, RB), (native, JJ), (americans, NNS), (...   \n",
       "1  [[(go, VB), (home, NN), (youre, NN), (drunk, N...   \n",
       "2  [[(amazon, NN), (investigating, VBG), (chinese...   \n",
       "3  [[(someone, NN), (shouldvetaken, VBD), (piece,...   \n",
       "4  [[(obama, RB), (wanted, VBD), (liberals, NNS),...   \n",
       "\n",
       "                                     tweet_tokens_sw  \\\n",
       "0                [[ask, native, americans, take, .]]   \n",
       "1        [[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]   \n",
       "2  [[amazon, investigating, chinese, employees, s...   \n",
       "3  [[someone, shouldvetaken, piece, shit, volcano...   \n",
       "4  [[obama, wanted, liberals, &, illegals, move, ...   \n",
       "\n",
       "                                        tweet_tokens  \\\n",
       "0  [[she, should, ask, a, few, native, americans,...   \n",
       "1        [[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]   \n",
       "2  [[amazon, is, investigating, chinese, employee...   \n",
       "3  [[someone, shouldvetaken, this, piece, of, shi...   \n",
       "4  [[obama, wanted, liberals, &, illegals, to, mo...   \n",
       "\n",
       "                                   tweet_normalizado  \\\n",
       "0   she should ask a few native americans what th...   \n",
       "1                    go home youre drunk!      üëäüá∫üá∏üëä    \n",
       "2  amazon is investigating chinese employees who ...   \n",
       "3   someone shouldvetaken this piece of shit to a...   \n",
       "4    obama wanted liberals &amp; illegals to move...   \n",
       "\n",
       "                                               tweet     id subtask_a  \\\n",
       "0  @USER She should ask a few native Americans wh...  86426       OFF   \n",
       "1  @USER @USER Go home you‚Äôre drunk!!! @USER #MAG...  90194       OFF   \n",
       "2  Amazon is investigating Chinese employees who ...  16820       NOT   \n",
       "3  @USER Someone should'veTaken\" this piece of sh...  62688       OFF   \n",
       "4  @USER @USER Obama wanted liberals &amp; illega...  43605       NOT   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT       NaN  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "3       UNT       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pos_tagger(sent_tokens):\n",
    "    \n",
    "    sent_tagged = []\n",
    "    for token in sent_tokens:\n",
    "        sent_tagged.append(nltk.pos_tag(token))\n",
    "\n",
    "    return sent_tagged\n",
    "\n",
    "tweets['tweet_tagged'] = tweets['tweet_tokens_sw'].apply(pos_tagger)\n",
    "tweets[tweets.columns[::-1]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Lemmatiza√ß√£o\n",
    "\n",
    "entrada: tweet_tagged | sa√≠da: tweet_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAPEANDO OS POS-TAGS DO NLTK PARA O FORMATO ACEITO PELO WORDNET LEMMATIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict = {\"J\": wn.ADJ,\n",
    "            \"N\": wn.NOUN,\n",
    "            \"V\": wn.VERB,\n",
    "            \"R\": wn.ADV}\n",
    "\n",
    "def extract_wnpostag_from_postag(tag):\n",
    "    \n",
    "    #pega a primeira letra da tag\n",
    "    #segundo par√¢metro opcional caso haja chave ausente no dicion√°rio \n",
    "    return tag_dict.get(tag[0].upper(), None)\n",
    "\n",
    "def lemmatize_tupla_word_postag(tupla):\n",
    "    \n",
    "    #retorna uma tupla na forma (wordString, posTagString) \n",
    "    #como ('guitar', 'NN'), retorna a palavra lematizada.\n",
    "    tag = extract_wnpostag_from_postag(tupla[1])    \n",
    "    return lemmatizer.lemmatize(tupla[0], tag) if tag is not None else tupla[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEMMATIZANDO OS TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_lemma</th>\n",
       "      <th>tweet_tagged</th>\n",
       "      <th>tweet_tokens_sw</th>\n",
       "      <th>tweet_tokens</th>\n",
       "      <th>tweet_normalizado</th>\n",
       "      <th>tweet</th>\n",
       "      <th>id</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[ask, native, american, take, .]]</td>\n",
       "      <td>[[(ask, RB), (native, JJ), (americans, NNS), (...</td>\n",
       "      <td>[[ask, native, americans, take, .]]</td>\n",
       "      <td>[[she, should, ask, a, few, native, americans,...</td>\n",
       "      <td>she should ask a few native americans what th...</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>86426</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]</td>\n",
       "      <td>[[(go, VB), (home, NN), (youre, NN), (drunk, N...</td>\n",
       "      <td>[[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]</td>\n",
       "      <td>[[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]</td>\n",
       "      <td>go home youre drunk!      üëäüá∫üá∏üëä</td>\n",
       "      <td>@USER @USER Go home you‚Äôre drunk!!! @USER #MAG...</td>\n",
       "      <td>90194</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[amazon, investigate, chinese, employee, sell...</td>\n",
       "      <td>[[(amazon, NN), (investigating, VBG), (chinese...</td>\n",
       "      <td>[[amazon, investigating, chinese, employees, s...</td>\n",
       "      <td>[[amazon, is, investigating, chinese, employee...</td>\n",
       "      <td>amazon is investigating chinese employees who ...</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>16820</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[someone, shouldvetaken, piece, shit, volcano...</td>\n",
       "      <td>[[(someone, NN), (shouldvetaken, VBD), (piece,...</td>\n",
       "      <td>[[someone, shouldvetaken, piece, shit, volcano...</td>\n",
       "      <td>[[someone, shouldvetaken, this, piece, of, shi...</td>\n",
       "      <td>someone shouldvetaken this piece of shit to a...</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>62688</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[obama, want, liberal, &amp;, illegals, move, red...</td>\n",
       "      <td>[[(obama, RB), (wanted, VBD), (liberals, NNS),...</td>\n",
       "      <td>[[obama, wanted, liberals, &amp;, illegals, move, ...</td>\n",
       "      <td>[[obama, wanted, liberals, &amp;, illegals, to, mo...</td>\n",
       "      <td>obama wanted liberals &amp;amp; illegals to move...</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>43605</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         tweet_lemma  \\\n",
       "0                 [[ask, native, american, take, .]]   \n",
       "1        [[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]   \n",
       "2  [[amazon, investigate, chinese, employee, sell...   \n",
       "3  [[someone, shouldvetaken, piece, shit, volcano...   \n",
       "4  [[obama, want, liberal, &, illegals, move, red...   \n",
       "\n",
       "                                        tweet_tagged  \\\n",
       "0  [[(ask, RB), (native, JJ), (americans, NNS), (...   \n",
       "1  [[(go, VB), (home, NN), (youre, NN), (drunk, N...   \n",
       "2  [[(amazon, NN), (investigating, VBG), (chinese...   \n",
       "3  [[(someone, NN), (shouldvetaken, VBD), (piece,...   \n",
       "4  [[(obama, RB), (wanted, VBD), (liberals, NNS),...   \n",
       "\n",
       "                                     tweet_tokens_sw  \\\n",
       "0                [[ask, native, americans, take, .]]   \n",
       "1        [[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]   \n",
       "2  [[amazon, investigating, chinese, employees, s...   \n",
       "3  [[someone, shouldvetaken, piece, shit, volcano...   \n",
       "4  [[obama, wanted, liberals, &, illegals, move, ...   \n",
       "\n",
       "                                        tweet_tokens  \\\n",
       "0  [[she, should, ask, a, few, native, americans,...   \n",
       "1        [[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]   \n",
       "2  [[amazon, is, investigating, chinese, employee...   \n",
       "3  [[someone, shouldvetaken, this, piece, of, shi...   \n",
       "4  [[obama, wanted, liberals, &, illegals, to, mo...   \n",
       "\n",
       "                                   tweet_normalizado  \\\n",
       "0   she should ask a few native americans what th...   \n",
       "1                    go home youre drunk!      üëäüá∫üá∏üëä    \n",
       "2  amazon is investigating chinese employees who ...   \n",
       "3   someone shouldvetaken this piece of shit to a...   \n",
       "4    obama wanted liberals &amp; illegals to move...   \n",
       "\n",
       "                                               tweet     id subtask_a  \\\n",
       "0  @USER She should ask a few native Americans wh...  86426       OFF   \n",
       "1  @USER @USER Go home you‚Äôre drunk!!! @USER #MAG...  90194       OFF   \n",
       "2  Amazon is investigating Chinese employees who ...  16820       NOT   \n",
       "3  @USER Someone should'veTaken\" this piece of sh...  62688       OFF   \n",
       "4  @USER @USER Obama wanted liberals &amp; illega...  43605       NOT   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT       NaN  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "3       UNT       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize_tweets(sent_list):\n",
    "    \n",
    "    out_list = []\n",
    "    for token in sent_list:\n",
    "        lemmas = [lemmatize_tupla_word_postag(x) for x in token]\n",
    "        out_list.append(lemmas)\n",
    "\n",
    "    return out_list\n",
    "\n",
    "tweets['tweet_lemma'] = tweets['tweet_tagged'].apply(lemmatize_tweets)\n",
    "tweets[tweets.columns[::-1]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Chunking\n",
    "\n",
    "entrada: tweet_tagged | sa√≠da: tweet_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "\n",
    "#with redirect_stdout(open(os.devnull, \"w\")):\n",
    "    #nltk.download('maxent_ne_chunker')\n",
    "    #nltk.download('words')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_chunked</th>\n",
       "      <th>tweet_lemma</th>\n",
       "      <th>tweet_tagged</th>\n",
       "      <th>tweet_tokens_sw</th>\n",
       "      <th>tweet_tokens</th>\n",
       "      <th>tweet_normalizado</th>\n",
       "      <th>tweet</th>\n",
       "      <th>id</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[(ask, RB, O), (native, JJ, B-NP), (americans...</td>\n",
       "      <td>[[ask, native, american, take, .]]</td>\n",
       "      <td>[[(ask, RB), (native, JJ), (americans, NNS), (...</td>\n",
       "      <td>[[ask, native, americans, take, .]]</td>\n",
       "      <td>[[she, should, ask, a, few, native, americans,...</td>\n",
       "      <td>she should ask a few native americans what th...</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>86426</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[(go, VB, O), (home, NN, B-NP), (youre, NN, I...</td>\n",
       "      <td>[[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]</td>\n",
       "      <td>[[(go, VB), (home, NN), (youre, NN), (drunk, N...</td>\n",
       "      <td>[[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]</td>\n",
       "      <td>[[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]</td>\n",
       "      <td>go home youre drunk!      üëäüá∫üá∏üëä</td>\n",
       "      <td>@USER @USER Go home you‚Äôre drunk!!! @USER #MAG...</td>\n",
       "      <td>90194</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[(amazon, NN, B-NP), (investigating, VBG, O),...</td>\n",
       "      <td>[[amazon, investigate, chinese, employee, sell...</td>\n",
       "      <td>[[(amazon, NN), (investigating, VBG), (chinese...</td>\n",
       "      <td>[[amazon, investigating, chinese, employees, s...</td>\n",
       "      <td>[[amazon, is, investigating, chinese, employee...</td>\n",
       "      <td>amazon is investigating chinese employees who ...</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>16820</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[(someone, NN, B-NP), (shouldvetaken, VBD, O)...</td>\n",
       "      <td>[[someone, shouldvetaken, piece, shit, volcano...</td>\n",
       "      <td>[[(someone, NN), (shouldvetaken, VBD), (piece,...</td>\n",
       "      <td>[[someone, shouldvetaken, piece, shit, volcano...</td>\n",
       "      <td>[[someone, shouldvetaken, this, piece, of, shi...</td>\n",
       "      <td>someone shouldvetaken this piece of shit to a...</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>62688</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[(obama, RB, O), (wanted, VBD, O), (liberals,...</td>\n",
       "      <td>[[obama, want, liberal, &amp;, illegals, move, red...</td>\n",
       "      <td>[[(obama, RB), (wanted, VBD), (liberals, NNS),...</td>\n",
       "      <td>[[obama, wanted, liberals, &amp;, illegals, move, ...</td>\n",
       "      <td>[[obama, wanted, liberals, &amp;, illegals, to, mo...</td>\n",
       "      <td>obama wanted liberals &amp;amp; illegals to move...</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>43605</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       tweet_chunked  \\\n",
       "0  [[(ask, RB, O), (native, JJ, B-NP), (americans...   \n",
       "1  [[(go, VB, O), (home, NN, B-NP), (youre, NN, I...   \n",
       "2  [[(amazon, NN, B-NP), (investigating, VBG, O),...   \n",
       "3  [[(someone, NN, B-NP), (shouldvetaken, VBD, O)...   \n",
       "4  [[(obama, RB, O), (wanted, VBD, O), (liberals,...   \n",
       "\n",
       "                                         tweet_lemma  \\\n",
       "0                 [[ask, native, american, take, .]]   \n",
       "1        [[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]   \n",
       "2  [[amazon, investigate, chinese, employee, sell...   \n",
       "3  [[someone, shouldvetaken, piece, shit, volcano...   \n",
       "4  [[obama, want, liberal, &, illegals, move, red...   \n",
       "\n",
       "                                        tweet_tagged  \\\n",
       "0  [[(ask, RB), (native, JJ), (americans, NNS), (...   \n",
       "1  [[(go, VB), (home, NN), (youre, NN), (drunk, N...   \n",
       "2  [[(amazon, NN), (investigating, VBG), (chinese...   \n",
       "3  [[(someone, NN), (shouldvetaken, VBD), (piece,...   \n",
       "4  [[(obama, RB), (wanted, VBD), (liberals, NNS),...   \n",
       "\n",
       "                                     tweet_tokens_sw  \\\n",
       "0                [[ask, native, americans, take, .]]   \n",
       "1        [[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]   \n",
       "2  [[amazon, investigating, chinese, employees, s...   \n",
       "3  [[someone, shouldvetaken, piece, shit, volcano...   \n",
       "4  [[obama, wanted, liberals, &, illegals, move, ...   \n",
       "\n",
       "                                        tweet_tokens  \\\n",
       "0  [[she, should, ask, a, few, native, americans,...   \n",
       "1        [[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]   \n",
       "2  [[amazon, is, investigating, chinese, employee...   \n",
       "3  [[someone, shouldvetaken, this, piece, of, shi...   \n",
       "4  [[obama, wanted, liberals, &, illegals, to, mo...   \n",
       "\n",
       "                                   tweet_normalizado  \\\n",
       "0   she should ask a few native americans what th...   \n",
       "1                    go home youre drunk!      üëäüá∫üá∏üëä    \n",
       "2  amazon is investigating chinese employees who ...   \n",
       "3   someone shouldvetaken this piece of shit to a...   \n",
       "4    obama wanted liberals &amp; illegals to move...   \n",
       "\n",
       "                                               tweet     id subtask_a  \\\n",
       "0  @USER She should ask a few native Americans wh...  86426       OFF   \n",
       "1  @USER @USER Go home you‚Äôre drunk!!! @USER #MAG...  90194       OFF   \n",
       "2  Amazon is investigating Chinese employees who ...  16820       NOT   \n",
       "3  @USER Someone should'veTaken\" this piece of sh...  62688       OFF   \n",
       "4  @USER @USER Obama wanted liberals &amp; illega...  43605       NOT   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT       NaN  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "3       UNT       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "\n",
    "def chunker (tweets_list):\n",
    "    \n",
    "    pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "    pattern1 = 'NP: {<DT>?<JJ>*<NN.*>*}'\n",
    "    pattern2 = 'NP: {<DT><NN.*><.*>*<NN.*>}'\n",
    "    \n",
    "    out_list = []\n",
    "    \n",
    "    for lista in tweets_list:\n",
    "        cp = nltk.RegexpParser(pattern1)\n",
    "        cs = cp.parse(lista)\n",
    "        \n",
    "        iob_tagged = tree2conlltags(cs)\n",
    "            \n",
    "        out_list.append(iob_tagged) \n",
    "        \n",
    "    return out_list\n",
    "\n",
    "tweets['tweet_chunked'] = tweets['tweet_tagged'].apply(chunker)\n",
    "tweets[tweets.columns[::-1]].head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - NER - Reconhecimento de Entidades\n",
    "\n",
    "entrada: tweet_tagged | sa√≠da: tweet_NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "from nltk.chunk.regexp import ChunkString, ChunkRule, ChinkRule \n",
    "from nltk.tree import Tree \n",
    "from contextlib import redirect_stdout\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n",
      "Warning: parsing empty text\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_NER</th>\n",
       "      <th>tweet_chunked</th>\n",
       "      <th>tweet_lemma</th>\n",
       "      <th>tweet_tagged</th>\n",
       "      <th>tweet_tokens_sw</th>\n",
       "      <th>tweet_tokens</th>\n",
       "      <th>tweet_normalizado</th>\n",
       "      <th>tweet</th>\n",
       "      <th>id</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[(ask, RB, O), (native, JJ, B-NP), (americans...</td>\n",
       "      <td>[[(ask, RB, O), (native, JJ, B-NP), (americans...</td>\n",
       "      <td>[[ask, native, american, take, .]]</td>\n",
       "      <td>[[(ask, RB), (native, JJ), (americans, NNS), (...</td>\n",
       "      <td>[[ask, native, americans, take, .]]</td>\n",
       "      <td>[[she, should, ask, a, few, native, americans,...</td>\n",
       "      <td>she should ask a few native americans what th...</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>86426</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[(go, VB, O), (home, NN, B-NP), (youre, NN, I...</td>\n",
       "      <td>[[(go, VB, O), (home, NN, B-NP), (youre, NN, I...</td>\n",
       "      <td>[[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]</td>\n",
       "      <td>[[(go, VB), (home, NN), (youre, NN), (drunk, N...</td>\n",
       "      <td>[[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]</td>\n",
       "      <td>[[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]</td>\n",
       "      <td>go home youre drunk!      üëäüá∫üá∏üëä</td>\n",
       "      <td>@USER @USER Go home you‚Äôre drunk!!! @USER #MAG...</td>\n",
       "      <td>90194</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[(amazon, NN, B-NP), (investigating, VBG, O),...</td>\n",
       "      <td>[[(amazon, NN, B-NP), (investigating, VBG, O),...</td>\n",
       "      <td>[[amazon, investigate, chinese, employee, sell...</td>\n",
       "      <td>[[(amazon, NN), (investigating, VBG), (chinese...</td>\n",
       "      <td>[[amazon, investigating, chinese, employees, s...</td>\n",
       "      <td>[[amazon, is, investigating, chinese, employee...</td>\n",
       "      <td>amazon is investigating chinese employees who ...</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>16820</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[(someone, NN, B-NP), (shouldvetaken, VBD, O)...</td>\n",
       "      <td>[[(someone, NN, B-NP), (shouldvetaken, VBD, O)...</td>\n",
       "      <td>[[someone, shouldvetaken, piece, shit, volcano...</td>\n",
       "      <td>[[(someone, NN), (shouldvetaken, VBD), (piece,...</td>\n",
       "      <td>[[someone, shouldvetaken, piece, shit, volcano...</td>\n",
       "      <td>[[someone, shouldvetaken, this, piece, of, shi...</td>\n",
       "      <td>someone shouldvetaken this piece of shit to a...</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>62688</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[(obama, RB, O), (wanted, VBD, O), (liberals,...</td>\n",
       "      <td>[[(obama, RB, O), (wanted, VBD, O), (liberals,...</td>\n",
       "      <td>[[obama, want, liberal, &amp;, illegals, move, red...</td>\n",
       "      <td>[[(obama, RB), (wanted, VBD), (liberals, NNS),...</td>\n",
       "      <td>[[obama, wanted, liberals, &amp;, illegals, move, ...</td>\n",
       "      <td>[[obama, wanted, liberals, &amp;, illegals, to, mo...</td>\n",
       "      <td>obama wanted liberals &amp;amp; illegals to move...</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>43605</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           tweet_NER  \\\n",
       "0  [[(ask, RB, O), (native, JJ, B-NP), (americans...   \n",
       "1  [[(go, VB, O), (home, NN, B-NP), (youre, NN, I...   \n",
       "2  [[(amazon, NN, B-NP), (investigating, VBG, O),...   \n",
       "3  [[(someone, NN, B-NP), (shouldvetaken, VBD, O)...   \n",
       "4  [[(obama, RB, O), (wanted, VBD, O), (liberals,...   \n",
       "\n",
       "                                       tweet_chunked  \\\n",
       "0  [[(ask, RB, O), (native, JJ, B-NP), (americans...   \n",
       "1  [[(go, VB, O), (home, NN, B-NP), (youre, NN, I...   \n",
       "2  [[(amazon, NN, B-NP), (investigating, VBG, O),...   \n",
       "3  [[(someone, NN, B-NP), (shouldvetaken, VBD, O)...   \n",
       "4  [[(obama, RB, O), (wanted, VBD, O), (liberals,...   \n",
       "\n",
       "                                         tweet_lemma  \\\n",
       "0                 [[ask, native, american, take, .]]   \n",
       "1        [[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]   \n",
       "2  [[amazon, investigate, chinese, employee, sell...   \n",
       "3  [[someone, shouldvetaken, piece, shit, volcano...   \n",
       "4  [[obama, want, liberal, &, illegals, move, red...   \n",
       "\n",
       "                                        tweet_tagged  \\\n",
       "0  [[(ask, RB), (native, JJ), (americans, NNS), (...   \n",
       "1  [[(go, VB), (home, NN), (youre, NN), (drunk, N...   \n",
       "2  [[(amazon, NN), (investigating, VBG), (chinese...   \n",
       "3  [[(someone, NN), (shouldvetaken, VBD), (piece,...   \n",
       "4  [[(obama, RB), (wanted, VBD), (liberals, NNS),...   \n",
       "\n",
       "                                     tweet_tokens_sw  \\\n",
       "0                [[ask, native, americans, take, .]]   \n",
       "1        [[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]   \n",
       "2  [[amazon, investigating, chinese, employees, s...   \n",
       "3  [[someone, shouldvetaken, piece, shit, volcano...   \n",
       "4  [[obama, wanted, liberals, &, illegals, move, ...   \n",
       "\n",
       "                                        tweet_tokens  \\\n",
       "0  [[she, should, ask, a, few, native, americans,...   \n",
       "1        [[go, home, youre, drunk, !], [üëä, üá∫, üá∏, üëä]]   \n",
       "2  [[amazon, is, investigating, chinese, employee...   \n",
       "3  [[someone, shouldvetaken, this, piece, of, shi...   \n",
       "4  [[obama, wanted, liberals, &, illegals, to, mo...   \n",
       "\n",
       "                                   tweet_normalizado  \\\n",
       "0   she should ask a few native americans what th...   \n",
       "1                    go home youre drunk!      üëäüá∫üá∏üëä    \n",
       "2  amazon is investigating chinese employees who ...   \n",
       "3   someone shouldvetaken this piece of shit to a...   \n",
       "4    obama wanted liberals &amp; illegals to move...   \n",
       "\n",
       "                                               tweet     id subtask_a  \\\n",
       "0  @USER She should ask a few native Americans wh...  86426       OFF   \n",
       "1  @USER @USER Go home you‚Äôre drunk!!! @USER #MAG...  90194       OFF   \n",
       "2  Amazon is investigating Chinese employees who ...  16820       NOT   \n",
       "3  @USER Someone should'veTaken\" this piece of sh...  62688       OFF   \n",
       "4  @USER @USER Obama wanted liberals &amp; illega...  43605       NOT   \n",
       "\n",
       "  subtask_b subtask_c  \n",
       "0       UNT       NaN  \n",
       "1       TIN       IND  \n",
       "2       NaN       NaN  \n",
       "3       UNT       NaN  \n",
       "4       NaN       NaN  "
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tweet_NER(tweets_list):\n",
    "    \n",
    "    out_list = []\n",
    "    for tweet in tweets_list:\n",
    "        out_list.append(nltk.ne_chunk(tweet))\n",
    "        \n",
    "    return out_list\n",
    "\n",
    "tweets['tweet_NER'] = tweets['tweet_tagged'].apply(chunker)\n",
    "tweets[tweets.columns[::-1]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produtos da Atividade 01:\n",
    "\n",
    "As entregas da atividade 01 s√£o as listas:\n",
    "\n",
    "- tweet_lemma\n",
    "- tweet_chunked\n",
    "- tweet_NER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
