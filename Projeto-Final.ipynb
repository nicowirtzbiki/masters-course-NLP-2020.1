{
 "cells": [
  {
   "attachments": {
    "Unifor_logo.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Unifor_logo.png](attachment:Unifor_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINERAÇÃO DE TEXTOS E DA WEB\n",
    "\n",
    "## ATIVIDADE 03\n",
    "\n",
    "#### GRUPO 02:\n",
    "- Nicole Wirtzbiki\n",
    "- Agenor Júnior\n",
    "- Torricelli Evangelista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------\n",
    "INICIO ATIVIDADE 03\n",
    "\n",
    "- 01. Vetor de features a partir do dataset de treinamento (BOW-Unigramas, BOW-bigramas, sinonimos, hiperonimos, tf-idf, word-embeddings). Obs: nao necessariamente todas as features; Podem ser testadas varias combinacoes de festures.\n",
    "\n",
    "- 02. Definição do Classificador (Naive Bayes, Logistic Regression, SVM, Random Forest). Obs: devem ser testados vários classificadores.\n",
    "\n",
    "- 03. Treinamento usando Cross-Validation para as duas sub-tarefas, com apresentacao da Matriz de Confusao e das metricas Precisão, Cobertura, F-Measure, de cada classe.\n",
    "\n",
    "Dia 17/07/2020 serão enviados às equipe o held-out dataset (a ser definido pelo professor), que será usado como benchmarking de avaliação das equipes.\n",
    "Obs: podem usar NLTK , SpaCy, Scikit Learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANTO PACOTES NECESSÁRIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import re # Importando o módulo \"REGEX\" para expressões regulares.\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('average_perceptron_tagger')\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV #com cross validation\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from empath import Empath\n",
    "\n",
    "from sklearn.model_selection import train_test_split,  cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------\n",
    "# 1 - CARREGANDO DATASET DE TREINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>UNT</td>\n",
       "      <td>OFF</td>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IND</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OFF</td>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT</td>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>UNT</td>\n",
       "      <td>OFF</td>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT</td>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13235</th>\n",
       "      <td>IND</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OFF</td>\n",
       "      <td>95338</td>\n",
       "      <td>@USER Sometimes I get strong vibes from people...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13236</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT</td>\n",
       "      <td>67210</td>\n",
       "      <td>Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13237</th>\n",
       "      <td>OTH</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OFF</td>\n",
       "      <td>82921</td>\n",
       "      <td>@USER And why report this garbage.  We don't g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13238</th>\n",
       "      <td>NaN</td>\n",
       "      <td>UNT</td>\n",
       "      <td>OFF</td>\n",
       "      <td>27429</td>\n",
       "      <td>@USER Pussy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13239</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT</td>\n",
       "      <td>46552</td>\n",
       "      <td>#Spanishrevenge vs. #justice #HumanRights and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13240 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subtask_c subtask_b subtask_a     id  \\\n",
       "0           NaN       UNT       OFF  86426   \n",
       "1           IND       TIN       OFF  90194   \n",
       "2           NaN       NaN       NOT  16820   \n",
       "3           NaN       UNT       OFF  62688   \n",
       "4           NaN       NaN       NOT  43605   \n",
       "...         ...       ...       ...    ...   \n",
       "13235       IND       TIN       OFF  95338   \n",
       "13236       NaN       NaN       NOT  67210   \n",
       "13237       OTH       TIN       OFF  82921   \n",
       "13238       NaN       UNT       OFF  27429   \n",
       "13239       NaN       NaN       NOT  46552   \n",
       "\n",
       "                                                   tweet  \n",
       "0      @USER She should ask a few native Americans wh...  \n",
       "1      @USER @USER Go home you’re drunk!!! @USER #MAG...  \n",
       "2      Amazon is investigating Chinese employees who ...  \n",
       "3      @USER Someone should'veTaken\" this piece of sh...  \n",
       "4      @USER @USER Obama wanted liberals &amp; illega...  \n",
       "...                                                  ...  \n",
       "13235  @USER Sometimes I get strong vibes from people...  \n",
       "13236  Benidorm ✅  Creamfields ✅  Maga ✅   Not too sh...  \n",
       "13237  @USER And why report this garbage.  We don't g...  \n",
       "13238                                        @USER Pussy  \n",
       "13239  #Spanishrevenge vs. #justice #HumanRights and ...  \n",
       "\n",
       "[13240 rows x 5 columns]"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#leitura para objeto dataframe\n",
    "tweets = pd.read_csv('/home/nico/Área de Trabalho/MineracaoDadosWeb/entrega 2/olid-training-v1.0.tsv', sep='\\t',encoding= 'utf-8')\n",
    "\n",
    "#conversão da coluna 'id' de inteiro para string\n",
    "tweets['id'] = tweets['id'].astype('str')\n",
    "\n",
    "#visualização dos primeiros registros. tweets de treino para subtask_a\n",
    "tweets = tweets[['subtask_c','subtask_b','subtask_a','id','tweet']]\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONTANDO AS CLASSIFICAÇÕES DAS SUBTAKS A E B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOT    8840\n",
       "OFF    4400\n",
       "Name: subtask_a, dtype: int64"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['subtask_a'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TIN    3876\n",
       "UNT     524\n",
       "Name: subtask_b, dtype: int64"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['subtask_b'].value_counts() #só os tweets que são OFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_map = {'NOT': 'inofensivo', 'OFF': 'ofensivo'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------\n",
    "# 2 - CARREGANDO OS DATASETS DE TESTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `testset_a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15923</td>\n",
       "      <td>#WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27014</td>\n",
       "      <td>#ConstitutionDay is revered by Conservatives, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30530</td>\n",
       "      <td>#FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13876</td>\n",
       "      <td>#Watching #Boomer getting the news that she is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60133</td>\n",
       "      <td>#NoPasaran: Unity demo to oppose the far-right...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>73439</td>\n",
       "      <td>#DespicableDems lie again about rifles. Dem Di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>25657</td>\n",
       "      <td>#MeetTheSpeakers 🙌 @USER will present in our e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>67018</td>\n",
       "      <td>3 people just unfollowed me for talking about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>50665</td>\n",
       "      <td>#WednesdayWisdom Antifa calls the right fascis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>24583</td>\n",
       "      <td>#Kavanaugh typical #liberals , #Democrats URL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>860 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet\n",
       "0    15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...\n",
       "1    27014  #ConstitutionDay is revered by Conservatives, ...\n",
       "2    30530  #FOXNews #NRA #MAGA #POTUS #TRUMP #2ndAmendmen...\n",
       "3    13876  #Watching #Boomer getting the news that she is...\n",
       "4    60133  #NoPasaran: Unity demo to oppose the far-right...\n",
       "..     ...                                                ...\n",
       "855  73439  #DespicableDems lie again about rifles. Dem Di...\n",
       "856  25657  #MeetTheSpeakers 🙌 @USER will present in our e...\n",
       "857  67018  3 people just unfollowed me for talking about ...\n",
       "858  50665  #WednesdayWisdom Antifa calls the right fascis...\n",
       "859  24583      #Kavanaugh typical #liberals , #Democrats URL\n",
       "\n",
       "[860 rows x 2 columns]"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_a = pd.read_csv('/home/nico/Área de Trabalho/MineracaoDadosWeb/entrega 2/testset-levela.tsv', sep='\\t',encoding= 'utf-8')\n",
    "testset_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `testset_b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15923</td>\n",
       "      <td>#WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60133</td>\n",
       "      <td>#NoPasaran: Unity demo to oppose the far-right...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83681</td>\n",
       "      <td>. . . What the fuck did he do this time?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65507</td>\n",
       "      <td>@USER Do you get the feeling he is kissing @US...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12588</td>\n",
       "      <td>@USER Nigga ware da hits at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>22569</td>\n",
       "      <td>#Antifa are mentally unstable cowards, pretend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>48938</td>\n",
       "      <td>@USER @USER And Browning looked like dog shit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>41438</td>\n",
       "      <td>All two of them taste like ass. URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>73439</td>\n",
       "      <td>#DespicableDems lie again about rifles. Dem Di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>67018</td>\n",
       "      <td>3 people just unfollowed me for talking about ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                              tweet\n",
       "0    15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...\n",
       "1    60133  #NoPasaran: Unity demo to oppose the far-right...\n",
       "2    83681           . . . What the fuck did he do this time?\n",
       "3    65507  @USER Do you get the feeling he is kissing @US...\n",
       "4    12588                        @USER Nigga ware da hits at\n",
       "..     ...                                                ...\n",
       "235  22569  #Antifa are mentally unstable cowards, pretend...\n",
       "236  48938  @USER @USER And Browning looked like dog shit ...\n",
       "237  41438                All two of them taste like ass. URL\n",
       "238  73439  #DespicableDems lie again about rifles. Dem Di...\n",
       "239  67018  3 people just unfollowed me for talking about ...\n",
       "\n",
       "[240 rows x 2 columns]"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_b = pd.read_csv('/home/nico/Área de Trabalho/MineracaoDadosWeb/entrega 2/testset-levelb.tsv', sep='\\t',encoding= 'utf-8')\n",
    "testset_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------\n",
    "# 3 - CRIAÇÃO DE FEATURES DA LINGUAGEM\n",
    "FEATURES CRIADAS MANUALMENTE COM INFORMAÇÕES RETIRADAS DE CONTAGENS NO CORPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 - CONTANDO PALAVRAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>UNT</td>\n",
       "      <td>OFF</td>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IND</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OFF</td>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT</td>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subtask_c subtask_b subtask_a     id  \\\n",
       "0       NaN       UNT       OFF  86426   \n",
       "1       IND       TIN       OFF  90194   \n",
       "2       NaN       NaN       NOT  16820   \n",
       "\n",
       "                                               tweet  word_count  \n",
       "0  @USER She should ask a few native Americans wh...          14  \n",
       "1  @USER @USER Go home you’re drunk!!! @USER #MAG...          11  \n",
       "2  Amazon is investigating Chinese employees who ...          27  "
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testset_a\n",
    "testset_a['word_count'] = testset_a['tweet'].apply(lambda x: len(str(x).split()))\n",
    "#testset_b\n",
    "testset_b['word_count'] = testset_b['tweet'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "#dataset de treino\n",
    "tweets['word_count'] = tweets['tweet'].apply(lambda x: len(str(x).split()))\n",
    "tweets[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 - CONTANDO CARACTERES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>UNT</td>\n",
       "      <td>OFF</td>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>14</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IND</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OFF</td>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>11</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT</td>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>27</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subtask_c subtask_b subtask_a     id  \\\n",
       "0       NaN       UNT       OFF  86426   \n",
       "1       IND       TIN       OFF  90194   \n",
       "2       NaN       NaN       NOT  16820   \n",
       "\n",
       "                                               tweet  word_count  char_counts  \n",
       "0  @USER She should ask a few native Americans wh...          14           71  \n",
       "1  @USER @USER Go home you’re drunk!!! @USER #MAG...          11           67  \n",
       "2  Amazon is investigating Chinese employees who ...          27          182  "
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testset_a\n",
    "testset_a['char_counts'] = testset_a['tweet'].apply(lambda x: len(x))\n",
    "#testset_b\n",
    "testset_b['char_counts'] = testset_b['tweet'].apply(lambda x: len(x))\n",
    "\n",
    "#dataset de treino\n",
    "tweets['char_counts'] = tweets['tweet'].apply(lambda x: len(x))\n",
    "tweets[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 - COMPRIMENTO MÉDIO DAS PALAVRAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_word_len(x):\n",
    "    words = x.split()\n",
    "    word_len = 0\n",
    "    for word in words:\n",
    "        word_len = word_len + len(word)\n",
    "    return word_len/len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_counts</th>\n",
       "      <th>avg_word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>UNT</td>\n",
       "      <td>OFF</td>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>14</td>\n",
       "      <td>71</td>\n",
       "      <td>4.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IND</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OFF</td>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>11</td>\n",
       "      <td>67</td>\n",
       "      <td>5.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT</td>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>27</td>\n",
       "      <td>182</td>\n",
       "      <td>5.777778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subtask_c subtask_b subtask_a     id  \\\n",
       "0       NaN       UNT       OFF  86426   \n",
       "1       IND       TIN       OFF  90194   \n",
       "2       NaN       NaN       NOT  16820   \n",
       "\n",
       "                                               tweet  word_count  char_counts  \\\n",
       "0  @USER She should ask a few native Americans wh...          14           71   \n",
       "1  @USER @USER Go home you’re drunk!!! @USER #MAG...          11           67   \n",
       "2  Amazon is investigating Chinese employees who ...          27          182   \n",
       "\n",
       "   avg_word_len  \n",
       "0      4.142857  \n",
       "1      5.181818  \n",
       "2      5.777778  "
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testset_a\n",
    "testset_a['avg_word_len'] = testset_a['tweet'].apply(lambda x: get_avg_word_len(x))\n",
    "#testset_b\n",
    "testset_b['avg_word_len'] = testset_b['tweet'].apply(lambda x: get_avg_word_len(x))\n",
    "\n",
    "#dataset de treino\n",
    "tweets['avg_word_len'] = tweets['tweet'].apply(lambda x: get_avg_word_len(x))\n",
    "tweets[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 - CONTANDO STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_counts</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>stop_words_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>UNT</td>\n",
       "      <td>OFF</td>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>14</td>\n",
       "      <td>71</td>\n",
       "      <td>4.142857</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IND</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OFF</td>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>11</td>\n",
       "      <td>67</td>\n",
       "      <td>5.181818</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT</td>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>27</td>\n",
       "      <td>182</td>\n",
       "      <td>5.777778</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subtask_c subtask_b subtask_a     id  \\\n",
       "0       NaN       UNT       OFF  86426   \n",
       "1       IND       TIN       OFF  90194   \n",
       "2       NaN       NaN       NOT  16820   \n",
       "\n",
       "                                               tweet  word_count  char_counts  \\\n",
       "0  @USER She should ask a few native Americans wh...          14           71   \n",
       "1  @USER @USER Go home you’re drunk!!! @USER #MAG...          11           67   \n",
       "2  Amazon is investigating Chinese employees who ...          27          182   \n",
       "\n",
       "   avg_word_len  stop_words_len  \n",
       "0      4.142857               8  \n",
       "1      5.181818               0  \n",
       "2      5.777778               8  "
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testset_a\n",
    "testset_a['stop_words_len'] = testset_a['tweet'].apply(lambda x: len([t for t in x.split() if t in STOP_WORDS]))\n",
    "#testset_b\n",
    "testset_b['stop_words_len'] = testset_b['tweet'].apply(lambda x: len([t for t in x.split() if t in STOP_WORDS]))\n",
    "\n",
    "#dataset de treino\n",
    "tweets['stop_words_len'] = tweets['tweet'].apply(lambda x: len([t for t in x.split() if t in STOP_WORDS]))\n",
    "tweets[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 - CONTANDO #HASHTAGS E @MENTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_counts</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>stop_words_len</th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>mentions_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>UNT</td>\n",
       "      <td>OFF</td>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>14</td>\n",
       "      <td>71</td>\n",
       "      <td>4.142857</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IND</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OFF</td>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>11</td>\n",
       "      <td>67</td>\n",
       "      <td>5.181818</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT</td>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>27</td>\n",
       "      <td>182</td>\n",
       "      <td>5.777778</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subtask_c subtask_b subtask_a     id  \\\n",
       "0       NaN       UNT       OFF  86426   \n",
       "1       IND       TIN       OFF  90194   \n",
       "2       NaN       NaN       NOT  16820   \n",
       "\n",
       "                                               tweet  word_count  char_counts  \\\n",
       "0  @USER She should ask a few native Americans wh...          14           71   \n",
       "1  @USER @USER Go home you’re drunk!!! @USER #MAG...          11           67   \n",
       "2  Amazon is investigating Chinese employees who ...          27          182   \n",
       "\n",
       "   avg_word_len  stop_words_len  hashtags_count  mentions_count  \n",
       "0      4.142857               8               0               1  \n",
       "1      5.181818               0               2               3  \n",
       "2      5.777778               8               5               0  "
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testset_a\n",
    "testset_a['hashtags_count'] = testset_a['tweet'].apply(lambda x: len([t for t in x.split() if t.startswith('#')]))\n",
    "testset_a['mentions_count'] = testset_a['tweet'].apply(lambda x: len([t for t in x.split() if t.startswith('@')]))\n",
    "#testset_b\n",
    "testset_b['hashtags_count'] = testset_b['tweet'].apply(lambda x: len([t for t in x.split() if t.startswith('#')]))\n",
    "testset_b['mentions_count'] = testset_b['tweet'].apply(lambda x: len([t for t in x.split() if t.startswith('@')]))\n",
    "\n",
    "#dataset de treino\n",
    "tweets['hashtags_count'] = tweets['tweet'].apply(lambda x: len([t for t in x.split() if t.startswith('#')]))\n",
    "tweets['mentions_count'] = tweets['tweet'].apply(lambda x: len([t for t in x.split() if t.startswith('@')]))\n",
    "tweets[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 - CONTAGEM DE DIGITOS NUMÉRICOS NOS TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_counts</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>stop_words_len</th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>mentions_count</th>\n",
       "      <th>numerics_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT</td>\n",
       "      <td>42992</td>\n",
       "      <td>@USER What’s the difference between #Kavanaugh...</td>\n",
       "      <td>40</td>\n",
       "      <td>279</td>\n",
       "      <td>5.825000</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>GRP</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OFF</td>\n",
       "      <td>84102</td>\n",
       "      <td>4 out of 10 British people are basically full-...</td>\n",
       "      <td>20</td>\n",
       "      <td>124</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>OTH</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OFF</td>\n",
       "      <td>33853</td>\n",
       "      <td>@USER @USER The prison system is so fucked.  W...</td>\n",
       "      <td>39</td>\n",
       "      <td>222</td>\n",
       "      <td>4.692308</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subtask_c subtask_b subtask_a     id  \\\n",
       "11       NaN       NaN       NOT  42992   \n",
       "54       GRP       TIN       OFF  84102   \n",
       "59       OTH       TIN       OFF  33853   \n",
       "\n",
       "                                                tweet  word_count  \\\n",
       "11  @USER What’s the difference between #Kavanaugh...          40   \n",
       "54  4 out of 10 British people are basically full-...          20   \n",
       "59  @USER @USER The prison system is so fucked.  W...          39   \n",
       "\n",
       "    char_counts  avg_word_len  stop_words_len  hashtags_count  mentions_count  \\\n",
       "11          279      5.825000              13               9               2   \n",
       "54          124      5.250000               7               0               0   \n",
       "59          222      4.692308              17               0               2   \n",
       "\n",
       "    numerics_count  \n",
       "11               1  \n",
       "54               4  \n",
       "59               1  "
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testset_a\n",
    "testset_a['numerics_count'] = testset_a['tweet'].apply(lambda x: len([t for t in x.split() if t.isdigit()]))\n",
    "#testset_b\n",
    "testset_b['numerics_count'] = testset_b['tweet'].apply(lambda x: len([t for t in x.split() if t.isdigit()]))\n",
    "\n",
    "#dataset de treino\n",
    "tweets['numerics_count'] = tweets['tweet'].apply(lambda x: len([t for t in x.split() if t.isdigit()]))\n",
    "tweets[tweets['numerics_count']>0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 - CONTAGEM DE PALAVRAS EM CAIXA ALTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_counts</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>stop_words_len</th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>mentions_count</th>\n",
       "      <th>numerics_count</th>\n",
       "      <th>capital_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IND</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OFF</td>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>11</td>\n",
       "      <td>67</td>\n",
       "      <td>5.181818</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NOT</td>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>27</td>\n",
       "      <td>182</td>\n",
       "      <td>5.777778</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>IND</td>\n",
       "      <td>TIN</td>\n",
       "      <td>OFF</td>\n",
       "      <td>13384</td>\n",
       "      <td>@USER Canada doesn’t need another CUCK! We alr...</td>\n",
       "      <td>19</td>\n",
       "      <td>138</td>\n",
       "      <td>6.315789</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subtask_c subtask_b subtask_a     id  \\\n",
       "1       IND       TIN       OFF  90194   \n",
       "2       NaN       NaN       NOT  16820   \n",
       "9       IND       TIN       OFF  13384   \n",
       "\n",
       "                                               tweet  word_count  char_counts  \\\n",
       "1  @USER @USER Go home you’re drunk!!! @USER #MAG...          11           67   \n",
       "2  Amazon is investigating Chinese employees who ...          27          182   \n",
       "9  @USER Canada doesn’t need another CUCK! We alr...          19          138   \n",
       "\n",
       "   avg_word_len  stop_words_len  hashtags_count  mentions_count  \\\n",
       "1      5.181818               0               2               3   \n",
       "2      5.777778               8               5               0   \n",
       "9      6.315789               6               4               1   \n",
       "\n",
       "   numerics_count  capital_count  \n",
       "1               0              2  \n",
       "2               0              5  \n",
       "9               0              1  "
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testset_a\n",
    "testset_a['capital_count'] = testset_a['tweet'].apply(lambda x: len([t for t in x.split() if t.isupper() and t != '@USER']))\n",
    "#testset_b\n",
    "testset_b['capital_count'] = testset_b['tweet'].apply(lambda x: len([t for t in x.split() if t.isupper() and t != '@USER']))\n",
    "\n",
    "#dataset de treino\n",
    "tweets['capital_count'] = tweets['tweet'].apply(lambda x: len([t for t in x.split() if t.isupper() and t != '@USER']))\n",
    "tweets[tweets['capital_count']>0][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------\n",
    "# 4 - NORMALIZAÇÃO DOS TWEETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSAMENTO E LIMPEZA DOS DADOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 - TRANSFORMANDO EM CAIXA BAIXA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testset_a\n",
    "testset_a['tweet'] = testset_a['tweet'].apply(lambda x: x.lower())\n",
    "#testset_b\n",
    "testset_b['tweet'] = testset_b['tweet'].apply(lambda x: x.lower())\n",
    "\n",
    "#dataset de treino\n",
    "tweets['tweet'] = tweets['tweet'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 - REMOVER PONTUAÇÃO E CARACTERES ESPECIAIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testset_a\n",
    "testset_a['tweet'] = testset_a['tweet'].apply(lambda x: re.sub('[^A-Z a-z 0-9-]+', '', x))\n",
    "#testset_b\n",
    "testset_b['tweet'] = testset_b['tweet'].apply(lambda x: re.sub('[^A-Z a-z 0-9-]+', '', x))\n",
    "\n",
    "#dataset de treino\n",
    "tweets['tweet'] = tweets['tweet'].apply(lambda x: re.sub('[^A-Z a-z 0-9-]+', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 - REMOVER REPETIÇÕES DE \"@user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testset_a\n",
    "testset_a['tweet'] = testset_a['tweet'].apply(lambda x: re.sub('(user |user| user )+',' user ', x))\n",
    "#testset_b\n",
    "testset_b['tweet'] = testset_b['tweet'].apply(lambda x: re.sub('(user |user| user )+',' user ', x))\n",
    "\n",
    "#dataset de treino\n",
    "tweets['tweet'] = tweets['tweet'].apply(lambda x: re.sub('(user |user| user )+',' user ', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 - REMOVER ESPAÇOS MÚLTIPLOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#testset_a\n",
    "testset_a['tweet'] = testset_a['tweet'].apply(lambda x: \" \".join(x.split()))\n",
    "#testset_b\n",
    "testset_b['tweet'] = testset_b['tweet'].apply(lambda x: \" \".join(x.split()))\n",
    "\n",
    "#dataset de treino\n",
    "tweets['tweet'] = tweets['tweet'].apply(lambda x: \" \".join(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 - CORREÇÕES DE VOCABULÁRIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treat_text(text):\n",
    "    text = re.sub('url$', '', text, flags=re.MULTILINE) #remover url\n",
    "    text = re.sub(r'^n$', 'and', text, flags=re.MULTILINE) #correção de vocabulário\n",
    "    text = re.sub(r'^u$', 'you', text, flags=re.MULTILINE) #correção de vocabulário\n",
    "    text = re.sub(r'^r$', 'are', text, flags=re.MULTILINE) #correção de vocabulário\n",
    "    text = re.sub(r'^sh*t$', 'shit', text, flags=re.MULTILINE) #correção de vocabulário\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testset_a\n",
    "testset_a['tweet'] = testset_a['tweet'].apply(lambda x: treat_text(x))\n",
    "#testset_b\n",
    "testset_b['tweet'] = testset_b['tweet'].apply(lambda x: treat_text(x))\n",
    "\n",
    "#dataset de treino\n",
    "tweets['tweet'] = tweets['tweet'].apply(lambda x: treat_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6 - EXPANDIR PALAVRAS CONTRAÍDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "    \"whats\": \"what is\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what'd\": \"what would\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"dont\": \"do not\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"its\": \"it is\",\n",
    "    \"youre\": \"you are\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"youve\": \"you have\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"hes\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how does\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"im\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"shes\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"wanna\": \"want to\",\n",
    "    \"gonna\": \"going to\",\n",
    "    \"gotta\": \"have got to\",\n",
    "    \" u \": \" you \",\n",
    "    \" ur \": \" your \",\n",
    "    \" n \": \" and \"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_to_exp(x):\n",
    "    if type(x) is str:\n",
    "        for key in contractions:\n",
    "            value = contractions[key]\n",
    "            x = x.replace(key, value)\n",
    "        return x\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    user she should ask a few native americans wha...\n",
       "1      user go home you are drunk user maga trump2020 \n",
       "2    amazon is investigating chinese employees who ...\n",
       "3    user someone shouldvetaken this piece of shit ...\n",
       "4    user obama wanted liberals amp illegals to mov...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testset_a\n",
    "testset_a['tweet'] = testset_a['tweet'].apply(lambda x: cont_to_exp(x))\n",
    "#testset_b\n",
    "testset_b['tweet'] = testset_b['tweet'].apply(lambda x: cont_to_exp(x))\n",
    "\n",
    "#dataset de treino\n",
    "tweets['tweet'] = tweets['tweet'].apply(lambda x: cont_to_exp(x))\n",
    "tweets['tweet'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7 - REMOVER STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testset_a\n",
    "testset_a['tweet'] = testset_a['tweet'].apply(lambda x: \" \".join([t for t in x.split() if t not in STOP_WORDS]))\n",
    "#testset_b\n",
    "testset_b['tweet'] = testset_b['tweet'].apply(lambda x: \" \".join([t for t in x.split() if t not in STOP_WORDS]))\n",
    "\n",
    "#dataset de treino\n",
    "tweets['tweet'] = tweets['tweet'].apply(lambda x: \" \".join([t for t in x.split() if t not in STOP_WORDS]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            user ask native americans\n",
       "1                  user home drunk user maga trump2020\n",
       "2    amazon investigating chinese employees selling...\n",
       "3                user shouldvetaken piece shit volcano\n",
       "4    user obama wanted liberals amp illegals red st...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['tweet'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8 - POS TAGGER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(x):\n",
    "    x_list = []\n",
    "    doc = nlp(x)\n",
    "    for token in doc: \n",
    "        pos = (token, token.pos_) \n",
    "        x_list.append(pos)\n",
    "    return x_list\n",
    "    \n",
    "# You want list of Verb tokens \n",
    "#print(\"Verbs:\", [token.text for token in doc if token.pos_ == \"VERB\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testset_a\n",
    "#testset_a['pos_tag'] = testset_a['tweet'].apply(lambda x: pos_tag(x))\n",
    "#testset_b\n",
    "#testset_b['pos_tag'] = testset_b['tweet'].apply(lambda x: pos_tag(x))\n",
    "\n",
    "#dataset de treino\n",
    "#tweets['pos_tag'] = tweets['tweet'].apply(lambda x: pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets['pos_tag'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.9 - LEMMAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_to_base(x):\n",
    "    x_list = []\n",
    "    doc = nlp(x)\n",
    "    \n",
    "    for token in doc:\n",
    "        lemma = str(token.lemma_)\n",
    "        if lemma == '-PRON-' or lemma == 'be':\n",
    "            lemma = token.text\n",
    "        x_list.append(lemma)\n",
    "    return(\" \".join(x_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testset_a\n",
    "testset_a['tweet'] = testset_a['tweet'].apply(lambda x: make_to_base(x))\n",
    "#testset_b\n",
    "testset_b['tweet'] = testset_b['tweet'].apply(lambda x: make_to_base(x))\n",
    "\n",
    "#dataset de treino\n",
    "tweets['tweet'] = tweets['tweet'].apply(lambda x: make_to_base(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                user ask native americans\n",
       "1                      user home drunk user maga trump2020\n",
       "2        amazon investigate chinese employee sell inter...\n",
       "3                    user shouldvetaken piece shit volcano\n",
       "4            user obama want liberal amp illegal red state\n",
       "                               ...                        \n",
       "13235    user someti ame strong vibes people mans vibe ...\n",
       "13236              benidorm creamfields maga shabby summer\n",
       "13237                             user report garbage crap\n",
       "13238                                           user pussy\n",
       "13239    spanishrevenge vs justice humanrights freedomo...\n",
       "Name: tweet, Length: 13240, dtype: object"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.10 - REMOÇÃO DE PALAVRAS COMUNS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Separando as 10 palavras mais comuns em todos os tweets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user            13244\n",
       "liberal          1549\n",
       "gun              1525\n",
       "not              1253\n",
       "control          1235\n",
       "antifa           1167\n",
       "like             1148\n",
       "-                1083\n",
       "maga             1006\n",
       "conservative     1004\n",
       "dtype: int64"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_common_words(x):\n",
    "    text = ' '.join(x)\n",
    "    text = text.split()\n",
    "    freq_comm = pd.Series(text).value_counts()\n",
    "    f10 = freq_comm[:10]\n",
    "    return f10\n",
    "\n",
    "#testset_a\n",
    "testset_a_f10 = get_common_words(testset_a['tweet'])\n",
    "#testset_b\n",
    "testset_b_f10 = get_common_words(testset_b['tweet'])\n",
    "\n",
    "#dataset de treino\n",
    "tweets_f10 = get_common_words(tweets['tweet'])\n",
    "tweets_f10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Retirando as 10 palavras mais comuns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testset_a\n",
    "testset_a['tweet'] = testset_a['tweet'].apply(lambda x: \" \".join(t for t in x.split() if t not in testset_a_f10))\n",
    "#testset_b\n",
    "testset_b['tweet'] = testset_b['tweet'].apply(lambda x: \" \".join(t for t in x.split() if t not in testset_b_f10))\n",
    "\n",
    "#dataset de treino\n",
    "tweets['tweet'] = tweets['tweet'].apply(lambda x: \" \".join(t for t in x.split() if t not in tweets_f10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.11 - REMOÇÃO DE PALAVRAS RARAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Separando as palavras raras em todos os tweets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rare_words(x):\n",
    "    text = ' '.join(x)\n",
    "    text = text.split()\n",
    "    freq_comm = pd.Series(text).value_counts()\n",
    "    rare_words = freq_comm[freq_comm.values == 1]\n",
    "    return rare_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dgps2018                1\n",
       "intellect               1\n",
       "ripoff                  1\n",
       "lmfaoooooo              1\n",
       "doingsnon               1\n",
       "                       ..\n",
       "andnevervotedemocrat    1\n",
       "governing               1\n",
       "zeland                  1\n",
       "brilliancy              1\n",
       "idiotalert              1\n",
       "Length: 10201, dtype: int64"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testset_a\n",
    "testset_a_rare = get_rare_words(testset_a['tweet'])\n",
    "#testset_b\n",
    "testset_b_rare = get_rare_words(testset_b['tweet'])\n",
    "\n",
    "#dataset de treino\n",
    "tweets_rare = get_rare_words(tweets['tweet'])\n",
    "tweets_rare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Retirando as palavras raras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testset_a\n",
    "testset_a['tweet'] = testset_a['tweet'].apply(lambda x: ' '.join([t for t in x.split() if t not in testset_a_rare]))\n",
    "#testset_b\n",
    "testset_b['tweet'] = testset_b['tweet'].apply(lambda x: ' '.join([t for t in x.split() if t not in testset_b_rare]))\n",
    "\n",
    "#dataset de treino\n",
    "tweets['tweet'] = tweets['tweet'].apply(lambda x: ' '.join([t for t in x.split() if t not in tweets_rare]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------\n",
    "# 5 - FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 - FEATURES DA LINGUAGEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13240, 13)"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(860, 10)"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 10)"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_counts</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>stop_words_len</th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>mentions_count</th>\n",
       "      <th>numerics_count</th>\n",
       "      <th>capital_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>71</td>\n",
       "      <td>4.142857</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>67</td>\n",
       "      <td>5.181818</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>182</td>\n",
       "      <td>5.777778</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>65</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>72</td>\n",
       "      <td>5.083333</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13235</th>\n",
       "      <td>25</td>\n",
       "      <td>129</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13236</th>\n",
       "      <td>12</td>\n",
       "      <td>62</td>\n",
       "      <td>3.916667</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13237</th>\n",
       "      <td>11</td>\n",
       "      <td>57</td>\n",
       "      <td>4.181818</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13238</th>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13239</th>\n",
       "      <td>30</td>\n",
       "      <td>228</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13240 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_count  char_counts  avg_word_len  stop_words_len  hashtags_count  \\\n",
       "0              14           71      4.142857               8               0   \n",
       "1              11           67      5.181818               0               2   \n",
       "2              27          182      5.777778               8               5   \n",
       "3              11           65      5.000000               4               0   \n",
       "4              12           72      5.083333               3               0   \n",
       "...           ...          ...           ...             ...             ...   \n",
       "13235          25          129      4.200000              11               0   \n",
       "13236          12           62      3.916667               3               0   \n",
       "13237          11           57      4.181818               4               0   \n",
       "13238           2           11      5.000000               0               0   \n",
       "13239          30          228      6.600000               3               9   \n",
       "\n",
       "       mentions_count  numerics_count  capital_count  \n",
       "0                   1               0              0  \n",
       "1                   3               0              2  \n",
       "2                   0               0              5  \n",
       "3                   1               0              0  \n",
       "4                   2               0              0  \n",
       "...               ...             ...            ...  \n",
       "13235               1               0              2  \n",
       "13236               0               0              0  \n",
       "13237               1               0              0  \n",
       "13238               1               0              0  \n",
       "13239              16               0              1  \n",
       "\n",
       "[13240 rows x 8 columns]"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testset_a\n",
    "manual_features_a = testset_a.drop(labels=['id','tweet'], axis = 1).reset_index(drop=True)\n",
    "#testset_b\n",
    "manual_features_b = testset_b.drop(labels=['id','tweet'], axis = 1).reset_index(drop=True)\n",
    "\n",
    "#dataset de treino a\n",
    "manual_features = tweets.drop(labels=['subtask_a','subtask_b','subtask_c','id','tweet'], axis = 1).reset_index(drop=True)\n",
    "manual_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_counts</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>stop_words_len</th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>mentions_count</th>\n",
       "      <th>numerics_count</th>\n",
       "      <th>capital_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>71</td>\n",
       "      <td>4.142857</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>67</td>\n",
       "      <td>5.181818</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>65</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>32</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_count  char_counts  avg_word_len  stop_words_len  hashtags_count  \\\n",
       "0          14           71      4.142857               8               0   \n",
       "1          11           67      5.181818               0               2   \n",
       "2          11           65      5.000000               4               0   \n",
       "3           6           33      4.666667               2               0   \n",
       "4           6           32      4.500000               0               0   \n",
       "\n",
       "   mentions_count  numerics_count  capital_count  \n",
       "0               1               0              0  \n",
       "1               3               0              2  \n",
       "2               1               0              0  \n",
       "3               1               0              0  \n",
       "4               2               0              0  "
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset de treino subtask_b\n",
    "tweets_b = tweets.dropna(subset=['subtask_b'])\n",
    "manual_features_tb = tweets_b.drop(labels=['subtask_a','subtask_b','subtask_c','id','tweet'], axis = 1).reset_index(drop=True)\n",
    "manual_features_tb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 - BOW UNIGRAMA DE VETORES DE FREQUÊNCIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvz = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARIO_TWEETS = cvz.fit(pd.concat([testset_a['tweet'],\n",
    "                                        testset_b['tweet'],\n",
    "                                        tweets['tweet']])).vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>005</th>\n",
       "      <th>06</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>090818</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>100000</th>\n",
       "      <th>100s</th>\n",
       "      <th>...</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zealot</th>\n",
       "      <th>zeetvserials</th>\n",
       "      <th>zero</th>\n",
       "      <th>zi</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zuckerberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   005  06  08  09  090818  10  100  1000  100000  100s  ...  zealand  zealot  \\\n",
       "0    0   0   0   0       0   0    0     0       0     0  ...        0       0   \n",
       "1    0   0   0   0       0   0    0     0       0     0  ...        0       0   \n",
       "2    0   0   0   0       0   0    0     0       0     0  ...        0       0   \n",
       "3    0   0   0   0       0   0    0     0       0     0  ...        0       0   \n",
       "4    0   0   0   0       0   0    0     0       0     0  ...        0       0   \n",
       "\n",
       "   zeetvserials  zero  zi  zionist  zombie  zone  zones  zuckerberg  \n",
       "0             0     0   0        0       0     0      0           0  \n",
       "1             0     0   0        0       0     0      0           0  \n",
       "2             0     0   0        0       0     0      0           0  \n",
       "3             0     0   0        0       0     0      0           0  \n",
       "4             0     0   0        0       0     0      0           0  \n",
       "\n",
       "[5 rows x 7512 columns]"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testset_a\n",
    "text_counts_a = cvz.transform(testset_a['tweet']) #guarda BOW em text_counts\n",
    "dfr_bow_a = pd.DataFrame(text_counts_a.toarray(), columns=cvz.get_feature_names())\n",
    "#testset_b\n",
    "text_counts_b = cvz.transform(testset_b['tweet']) #guarda BOW em text_counts\n",
    "dfr_bow_b = pd.DataFrame(text_counts_b.toarray(), columns=cvz.get_feature_names())\n",
    "dfr_bow_b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>005</th>\n",
       "      <th>06</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>090818</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>100000</th>\n",
       "      <th>100s</th>\n",
       "      <th>...</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zealot</th>\n",
       "      <th>zeetvserials</th>\n",
       "      <th>zero</th>\n",
       "      <th>zi</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zuckerberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   005  06  08  09  090818  10  100  1000  100000  100s  ...  zealand  zealot  \\\n",
       "0    0   0   0   0       0   0    0     0       0     0  ...        0       0   \n",
       "1    0   0   0   0       0   0    0     0       0     0  ...        0       0   \n",
       "2    0   0   0   0       0   0    0     0       0     0  ...        0       0   \n",
       "3    0   0   0   0       0   0    0     0       0     0  ...        0       0   \n",
       "4    0   0   0   0       0   0    0     0       0     0  ...        0       0   \n",
       "\n",
       "   zeetvserials  zero  zi  zionist  zombie  zone  zones  zuckerberg  \n",
       "0             0     0   0        0       0     0      0           0  \n",
       "1             0     0   0        0       0     0      0           0  \n",
       "2             0     0   0        0       0     0      0           0  \n",
       "3             0     0   0        0       0     0      0           0  \n",
       "4             0     0   0        0       0     0      0           0  \n",
       "\n",
       "[5 rows x 7512 columns]"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset de treino subtask_a\n",
    "text_counts = cvz.transform(tweets['tweet']) #guarda BOW em text_counts\n",
    "dfr_bow = pd.DataFrame(text_counts.toarray(), columns=cvz.get_feature_names())\n",
    "dfr_bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>005</th>\n",
       "      <th>06</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>090818</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>1000</th>\n",
       "      <th>100000</th>\n",
       "      <th>100s</th>\n",
       "      <th>...</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zealot</th>\n",
       "      <th>zeetvserials</th>\n",
       "      <th>zero</th>\n",
       "      <th>zi</th>\n",
       "      <th>zionist</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zuckerberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   005  06  08  09  090818  10  100  1000  100000  100s  ...  zealand  zealot  \\\n",
       "0    0   0   0   0       0   0    0     0       0     0  ...        0       0   \n",
       "1    0   0   0   0       0   0    0     0       0     0  ...        0       0   \n",
       "2    0   0   0   0       0   0    0     0       0     0  ...        0       0   \n",
       "3    0   0   0   0       0   0    0     0       0     0  ...        0       0   \n",
       "4    0   0   0   0       0   0    0     0       0     0  ...        0       0   \n",
       "\n",
       "   zeetvserials  zero  zi  zionist  zombie  zone  zones  zuckerberg  \n",
       "0             0     0   0        0       0     0      0           0  \n",
       "1             0     0   0        0       0     0      0           0  \n",
       "2             0     0   0        0       0     0      0           0  \n",
       "3             0     0   0        0       0     0      0           0  \n",
       "4             0     0   0        0       0     0      0           0  \n",
       "\n",
       "[5 rows x 7512 columns]"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset de treino subtask_b\n",
    "text_counts_tb = cvz.transform(tweets_b['tweet']) #guarda BOW em text_counts_tb\n",
    "dfr_bow_tb = pd.DataFrame(text_counts_tb.toarray(), columns=cvz.get_feature_names())\n",
    "dfr_bow_tb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 - BOW BIGRAMA DE VETORES DE FREQUÊNCIA\n",
    "* HOUVE ERRO POR FALTA DE MEMÓRIA AO ALOCAR OS DATASETS DE TREINO, POR ISSO NÃO FOI UTILIZADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = CountVectorizer(ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARIO_TWEETS = cv2.fit(pd.concat([testset_a['tweet'],\n",
    "                                        testset_b['tweet'],\n",
    "                                        tweets['tweet']])).vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>005 ego</th>\n",
       "      <th>005 moment</th>\n",
       "      <th>06 kg</th>\n",
       "      <th>06 star</th>\n",
       "      <th>08 12</th>\n",
       "      <th>08 suppose</th>\n",
       "      <th>09 14</th>\n",
       "      <th>09 open</th>\n",
       "      <th>090818 2nd</th>\n",
       "      <th>090818 story</th>\n",
       "      <th>...</th>\n",
       "      <th>zone law</th>\n",
       "      <th>zone people</th>\n",
       "      <th>zone rational</th>\n",
       "      <th>zone ridiculous</th>\n",
       "      <th>zone shoot</th>\n",
       "      <th>zone tds</th>\n",
       "      <th>zone work</th>\n",
       "      <th>zones cities</th>\n",
       "      <th>zones cri</th>\n",
       "      <th>zuckerberg soros</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82402 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   005 ego  005 moment  06 kg  06 star  08 12  08 suppose  09 14  09 open  \\\n",
       "0        0           0      0        0      0           0      0        0   \n",
       "1        0           0      0        0      0           0      0        0   \n",
       "2        0           0      0        0      0           0      0        0   \n",
       "3        0           0      0        0      0           0      0        0   \n",
       "4        0           0      0        0      0           0      0        0   \n",
       "\n",
       "   090818 2nd  090818 story  ...  zone law  zone people  zone rational  \\\n",
       "0           0             0  ...         0            0              0   \n",
       "1           0             0  ...         0            0              0   \n",
       "2           0             0  ...         0            0              0   \n",
       "3           0             0  ...         0            0              0   \n",
       "4           0             0  ...         0            0              0   \n",
       "\n",
       "   zone ridiculous  zone shoot  zone tds  zone work  zones cities  zones cri  \\\n",
       "0                0           0         0          0             0          0   \n",
       "1                0           0         0          0             0          0   \n",
       "2                0           0         0          0             0          0   \n",
       "3                0           0         0          0             0          0   \n",
       "4                0           0         0          0             0          0   \n",
       "\n",
       "   zuckerberg soros  \n",
       "0                 0  \n",
       "1                 0  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  \n",
       "\n",
       "[5 rows x 82402 columns]"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testset_a\n",
    "text_counts_a = cv2.transform(testset_a['tweet']) #guarda BOW em text_counts\n",
    "dfr_bow2_a = pd.DataFrame(text_counts_a.toarray(), columns=cv2.get_feature_names())\n",
    "#testset_b\n",
    "text_counts_b = cv2.transform(testset_b['tweet']) #guarda BOW em text_counts\n",
    "dfr_bow2_b = pd.DataFrame(text_counts_b.toarray(), columns=cv2.get_feature_names())\n",
    "dfr_bow2_b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset de treino subtask_a\n",
    "#text_counts = cv2.transform(tweets['tweet']) #guarda BOW em text_counts\n",
    "#dfr_bow2 = pd.DataFrame(text_counts.toarray(), columns=cv2.get_feature_names())\n",
    "\n",
    "#dataset de treino subtask_b\n",
    "#text_counts_2b = cv2.transform(tweets_b['tweet']) #guarda BOW em text_counts2\n",
    "#dfr_bow_2b = pd.DataFrame(text_counts.toarray(), columns=cv2.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 - VETORIZAÇÃO TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_tweets = tfidf.fit(pd.concat([testset_a['tweet'],\n",
    "                                        testset_b['tweet'],\n",
    "                                        tweets['tweet']])).vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<13240x7512 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 101335 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testset_a\n",
    "tfidf_a = tfidf.transform(testset_a['tweet'])\n",
    "#testset_b\n",
    "tfidf_b = tfidf.transform(testset_b['tweet'])\n",
    "\n",
    "#dataset de treino subtask_a\n",
    "tfidf_tweets = tfidf.transform(tweets['tweet'])\n",
    "tfidf_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4400x7512 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 37547 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset de treino subtask_b\n",
    "tfidf_tweets_tb = tfidf.transform(tweets_b['tweet'])\n",
    "tfidf_tweets_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<240x7512 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1331 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 - WORD EMBEDDINGS: VETORES GLOVE COM SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vec(x):\n",
    "    doc = nlp(x)\n",
    "    return doc.vector.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 2s, sys: 97.4 ms, total: 2min 2s\n",
      "Wall time: 2min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        [[-0.22207999, -0.011213002, -0.42468032, -0.0...\n",
       "1        [[-0.15683, 0.11466, -0.21521668, -0.034432665...\n",
       "2        [[-0.19677131, 0.16656922, -0.007898822, 0.063...\n",
       "3        [[-0.48832, -0.069639996, -0.05943, 0.125265, ...\n",
       "4        [[-0.10551766, 0.13025834, 0.1919475, 0.029003...\n",
       "                               ...                        \n",
       "13235    [[-0.1268047, 0.1118146, -0.0778862, -0.083835...\n",
       "13236    [[0.39715502, 0.166096, 0.17001, 0.013473999, ...\n",
       "13237    [[-0.69181997, -0.13534333, -0.06272934, -0.08...\n",
       "13238    [[-0.67881, -0.75509, -0.25609, 0.044741, 0.25...\n",
       "13239    [[-0.211325, 0.017242752, 0.07692652, 0.034257...\n",
       "Name: vec, Length: 13240, dtype: object"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#testset_a\n",
    "testset_a['vec'] = testset_a['tweet'].apply(lambda x: get_vec(x))\n",
    "#testset_b\n",
    "testset_b['vec'] = testset_b['tweet'].apply(lambda x: get_vec(x))\n",
    "\n",
    "#dataset de treino subtask_a\n",
    "tweets['vec'] = tweets['tweet'].apply(lambda x: get_vec(x))\n",
    "tweets['vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-445-a0f6127aed3b>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_b['vec'] = tweets_b['tweet'].apply(lambda x: get_vec(x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        [[-0.22207999, -0.011213002, -0.42468032, -0.0...\n",
       "1        [[-0.15683, 0.11466, -0.21521668, -0.034432665...\n",
       "3        [[-0.48832, -0.069639996, -0.05943, 0.125265, ...\n",
       "5        [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
       "6        [[-0.405465, 0.09807774, -0.39310747, -0.13841...\n",
       "                               ...                        \n",
       "13223    [[-0.42084712, -0.053085677, -0.08722335, -0.0...\n",
       "13227    [[-0.3316825, -0.3737634, -0.26345024, 0.00013...\n",
       "13235    [[-0.1268047, 0.1118146, -0.0778862, -0.083835...\n",
       "13237    [[-0.69181997, -0.13534333, -0.06272934, -0.08...\n",
       "13238    [[-0.67881, -0.75509, -0.25609, 0.044741, 0.25...\n",
       "Name: vec, Length: 4400, dtype: object"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset de treino subtask_b\n",
    "tweets_b['vec'] = tweets_b['tweet'].apply(lambda x: get_vec(x))\n",
    "tweets_b['vec']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6 - EMPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = Empath() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testset_a\n",
    "emp_testa = testset_a['tweet'].apply(lexicon.analyze).apply(pd.Series)\n",
    "#testset_b\n",
    "emp_testb = testset_b['tweet'].apply(lexicon.analyze).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>help</th>\n",
       "      <th>office</th>\n",
       "      <th>dance</th>\n",
       "      <th>money</th>\n",
       "      <th>wedding</th>\n",
       "      <th>domestic_work</th>\n",
       "      <th>sleep</th>\n",
       "      <th>medical_emergency</th>\n",
       "      <th>cold</th>\n",
       "      <th>hate</th>\n",
       "      <th>...</th>\n",
       "      <th>weapon</th>\n",
       "      <th>children</th>\n",
       "      <th>monster</th>\n",
       "      <th>ocean</th>\n",
       "      <th>giving</th>\n",
       "      <th>contentment</th>\n",
       "      <th>writing</th>\n",
       "      <th>rural</th>\n",
       "      <th>positive_emotion</th>\n",
       "      <th>musical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13223</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13227</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13235</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13237</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13238</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4400 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       help  office  dance  money  wedding  domestic_work  sleep  \\\n",
       "0       0.0     0.0    0.0    0.0      0.0            0.0    0.0   \n",
       "1       0.0     0.0    0.0    0.0      0.0            1.0    0.0   \n",
       "3       0.0     0.0    0.0    0.0      0.0            0.0    0.0   \n",
       "5       0.0     0.0    0.0    0.0      0.0            0.0    0.0   \n",
       "6       0.0     0.0    0.0    0.0      0.0            0.0    0.0   \n",
       "...     ...     ...    ...    ...      ...            ...    ...   \n",
       "13223   0.0     0.0    0.0    0.0      0.0            0.0    0.0   \n",
       "13227   0.0     0.0    0.0    0.0      0.0            0.0    0.0   \n",
       "13235   0.0     0.0    0.0    0.0      0.0            0.0    0.0   \n",
       "13237   0.0     0.0    0.0    0.0      0.0            0.0    0.0   \n",
       "13238   0.0     0.0    0.0    0.0      0.0            0.0    0.0   \n",
       "\n",
       "       medical_emergency  cold  hate  ...  weapon  children  monster  ocean  \\\n",
       "0                    0.0   0.0   0.0  ...     0.0       0.0      0.0    0.0   \n",
       "1                    0.0   0.0   0.0  ...     0.0       0.0      0.0    0.0   \n",
       "3                    0.0   0.0   0.0  ...     0.0       0.0      0.0    0.0   \n",
       "5                    0.0   0.0   0.0  ...     0.0       0.0      0.0    0.0   \n",
       "6                    0.0   0.0   0.0  ...     0.0       0.0      0.0    0.0   \n",
       "...                  ...   ...   ...  ...     ...       ...      ...    ...   \n",
       "13223                0.0   1.0   2.0  ...     2.0       1.0      2.0    0.0   \n",
       "13227                0.0   0.0   0.0  ...     0.0       1.0      0.0    0.0   \n",
       "13235                0.0   0.0   0.0  ...     0.0       0.0      0.0    0.0   \n",
       "13237                0.0   0.0   0.0  ...     0.0       0.0      0.0    0.0   \n",
       "13238                0.0   0.0   0.0  ...     0.0       0.0      0.0    0.0   \n",
       "\n",
       "       giving  contentment  writing  rural  positive_emotion  musical  \n",
       "0         0.0          0.0      0.0    0.0               0.0      0.0  \n",
       "1         0.0          0.0      0.0    0.0               0.0      0.0  \n",
       "3         0.0          0.0      0.0    0.0               0.0      0.0  \n",
       "5         0.0          0.0      0.0    0.0               0.0      0.0  \n",
       "6         0.0          0.0      0.0    0.0               0.0      0.0  \n",
       "...       ...          ...      ...    ...               ...      ...  \n",
       "13223     0.0          1.0      0.0    0.0               0.0      0.0  \n",
       "13227     0.0          0.0      0.0    0.0               0.0      0.0  \n",
       "13235     0.0          0.0      0.0    0.0               0.0      0.0  \n",
       "13237     0.0          0.0      0.0    0.0               0.0      0.0  \n",
       "13238     0.0          0.0      0.0    0.0               0.0      0.0  \n",
       "\n",
       "[4400 rows x 194 columns]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset de treino subtask_a\n",
    "emp_ta = tweets['tweet'].apply(lexicon.analyze).apply(pd.Series)\n",
    "#dataset de treino subtask_b\n",
    "emp_tb = tweets_b['tweet'].apply(lexicon.analyze).apply(pd.Series)\n",
    "emp_tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------\n",
    "# 6 - MODELOS DE MACHINE LEARNING PARA CLASSIFICAÇÃO DOS TEXTOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 - DEFININDO OS ALGORITMOS DE MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "lgr = LogisticRegression(random_state=42, max_iter=200)\n",
    "svm = LinearSVC(random_state=42, max_iter=200)\n",
    "rfc = RandomForestClassifier(random_state=42, n_jobs=-1, n_estimators=200)\n",
    "#sgd = SGDClassifier(n_jobs=-1, random_state=42, max_iter=200)\n",
    "#lgrcv = LogisticRegressionCV(cv = 2, random_state=42, max_iter=1000)\n",
    "#knn = KNeighborsClassifier(n_neighbors=5)\n",
    "#ada = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = {'GNB': gnb, 'SGD': sgd, 'LGR': lgr, 'LGR-CV': lgrcv, 'SVM': svm, 'RFC': rfc, 'KNN': knn, 'ADA': ada}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['GNB', 'LGR', 'SVM', 'RFC'])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = {'GNB': gnb, 'LGR': lgr, 'SVM': svm, 'RFC': rfc}\n",
    "clf.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 - FUNÇÃO DE CLASSIFICAÇÃO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(X,y):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "    \n",
    "    for key in clf.keys():\n",
    "        clf[key].fit(X_train, y_train)\n",
    "        y_pred = clf[key].predict(X_test)\n",
    "        #ac = accuracy_score(y_test, y_pred)\n",
    "        ac = cross_val_score(clf[key], X, y, cv=5)\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        print(key, \" ---> \", 'precisão:', np.mean(ac), 'f1 score:', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------\n",
    "# 7 - SUBTASK A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 - DEFININDO O TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizador_label = LabelBinarizer()\n",
    "t = tweets['subtask_a']\n",
    "\n",
    "y = binarizador_label.fit_transform(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 MODELO COM BOW DE FREQUÊNCIA UNIGRAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB  --->  precisão: 0.4641238670694864 f1 score: 0.46400760129653407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGR  --->  precisão: 0.7571752265861027 f1 score: 0.696369210862462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "<ipython-input-105-89430fe00da1>:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf[key].fit(X_train, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM  --->  precisão: 0.7356495468277946 f1 score: 0.6938940651515502\n",
      "RFC  --->  precisão: 0.770392749244713 f1 score: 0.7265793342191571\n",
      "CPU times: user 11min 49s, sys: 21.2 s, total: 12min 10s\n",
      "Wall time: 3min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classify(dfr_bow, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(dfr_bow)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.44 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-555-26e0cc489d5d>:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model1.fit(X_train, y_train)\n",
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7574773413897281"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "model1 = RandomForestClassifier()\n",
    "model1.fit(X_train, y_train)\n",
    "#score_basic1 = model1.score(X_test, y_test)\n",
    "score_basic1 = cross_val_score(model1,dfr_bow,y,cv=5)\n",
    "score_cross1 = np.mean(score_basic1) #score melhorado pela cross validation\n",
    "\n",
    "\n",
    "#score_basic1\n",
    "score_cross1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[773   2]\n",
      " [102   3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94       775\n",
      "           1       0.60      0.03      0.05       105\n",
      "\n",
      "    accuracy                           0.88       880\n",
      "   macro avg       0.74      0.51      0.50       880\n",
      "weighted avg       0.85      0.88      0.83       880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Matriz de Confusão para Random Forest\n",
    "pred1 = model1.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred1)) #labels=[\"OFF\",\"NOT\"]\n",
    "print(classification_report(y_test, pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = NOT | 1 = OFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 -  MODELO COM FEATURES DA LINGUAGEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB  --->  precisão: 0.5060422960725075 f1 score: 0.5038001667569216\n",
      "LGR  --->  precisão: 0.6669184290030211 f1 score: 0.40117655263514146\n",
      "SVM  --->  precisão: 0.6680513595166163 f1 score: 0.40907488013770366\n",
      "RFC  --->  precisão: 0.629154078549849 f1 score: 0.5210585448691738\n",
      "CPU times: user 5.01 s, sys: 193 ms, total: 5.2 s\n",
      "Wall time: 1.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classify(manual_features, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUPPORT VECTOR MACHINE - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(manual_features)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6676737160120846"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = SVC(kernel='linear')\n",
    "model2.fit(X_train, y_train)\n",
    "score_basic2 = model2.score(X_test, y_test)\n",
    "#score_basic2 = cross_val_score(model2, X, y, cv=5)\n",
    "#score_cross2 = np.mean(score_basic2) #score melhorado pela cross validation\n",
    "\n",
    "\n",
    "score_basic2\n",
    "#score_cross2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1768    0]\n",
      " [ 880    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80      1768\n",
      "           1       0.00      0.00      0.00       880\n",
      "\n",
      "    accuracy                           0.67      2648\n",
      "   macro avg       0.33      0.50      0.40      2648\n",
      "weighted avg       0.45      0.67      0.53      2648\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Matriz de Confusão para SVM\n",
    "pred2 = model2.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred2)) #labels=[\"OFF\",\"NOT\"]\n",
    "print(classification_report(y_test, pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = NOT | 1 = OFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4 - MODELO COM BOW FREQUÊNCIA & FEATURES DA LINGUAGEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfr_bow.join(manual_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB  --->  precisão: 0.4641238670694864 f1 score: 0.46400760129653407\n",
      "LGR  --->  precisão: 0.7552870090634441 f1 score: 0.6949080939223572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM  --->  precisão: 0.7356495468277946 f1 score: 0.6942882570142506\n",
      "RFC  --->  precisão: 0.7688821752265861 f1 score: 0.6918429003021147\n",
      "CPU times: user 5min 21s, sys: 2.41 s, total: 5min 24s\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classify(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.770392749244713"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = RandomForestClassifier()\n",
    "model3.fit(X_train, y_train)\n",
    "#score_basic3 = model3.score(X_test, y_test)\n",
    "score_basic3 = cross_val_score(model3, X, y, cv=5)\n",
    "score_cross3 = np.mean(score_basic3) #score melhorado pela cross validation\n",
    "\n",
    "\n",
    "#score_basic3\n",
    "score_cross3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1685   83]\n",
      " [ 525  355]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NOT       0.76      0.95      0.85      1768\n",
      "         OFF       0.81      0.40      0.54       880\n",
      "\n",
      "    accuracy                           0.77      2648\n",
      "   macro avg       0.79      0.68      0.69      2648\n",
      "weighted avg       0.78      0.77      0.74      2648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Matriz de Confusão para Random Forest\n",
    "pred3 = model3.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred3)) #labels=[\"OFF\",\"NOT\"]\n",
    "print(classification_report(y_test, pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = NOT | 1 = OFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5 -  MODELO COM TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB  --->  precisão: 0.4746978851963746 f1 score: 0.47468102860955275\n",
      "LGR  --->  precisão: 0.7556646525679759 f1 score: 0.6834150732148008\n",
      "SVM  --->  precisão: 0.7307401812688822 f1 score: 0.6885106262226173\n",
      "RFC  --->  precisão: 0.773036253776435 f1 score: 0.713513547570636\n",
      "CPU times: user 14min 2s, sys: 2.82 s, total: 14min 5s\n",
      "Wall time: 3min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classify(pd.DataFrame(tfidf_tweets.toarray()),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(tfidf_tweets.toarray())\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-533-d5e0bc40872b>:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model4.fit(X_train, y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7722809667673716"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4 = RandomForestClassifier()\n",
    "model4.fit(X_train, y_train)\n",
    "#score_basic4 = model4.score(X_test, y_test)\n",
    "score_basic4 = cross_val_score(model4, X, y,cv=5)\n",
    "score_cross4 = np.mean(score_basic4) #score melhorado pela cross validation\n",
    "\n",
    "\n",
    "#score_basic4\n",
    "score_cross4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1627  141]\n",
      " [ 462  418]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.92      0.84      1768\n",
      "           1       0.75      0.47      0.58       880\n",
      "\n",
      "    accuracy                           0.77      2648\n",
      "   macro avg       0.76      0.70      0.71      2648\n",
      "weighted avg       0.77      0.77      0.76      2648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Matriz de Confusão para Random Forest\n",
    "pred4 = model4.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred4)) #labels=[\"OFF\",\"NOT\"]\n",
    "print(classification_report(y_test, pred4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = NOT | 1 = OFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.6 - MODELO COM GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W = np.concatenate(tweets['vec'].to_numpy(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13240, 300)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB  --->  precisão: 0.6219788519637462 f1 score: 0.6183842018009302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGR  --->  precisão: 0.7424471299093656 f1 score: 0.6707024970606512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM  --->  precisão: 0.7186555891238671 f1 score: 0.5920913578393637\n",
      "RFC  --->  precisão: 0.7488670694864048 f1 score: 0.662246387972433\n",
      "CPU times: user 40 s, sys: 901 ms, total: 40.9 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classify(pd.DataFrame(W), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(W)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7382930513595166"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5 = RandomForestClassifier()\n",
    "model5.fit(X_train, y_train)\n",
    "#score_basic5 = model5.score(X_test, y_test)\n",
    "score_basic5 = cross_val_score(model5,X,y,cv=5)\n",
    "score_cross5 = np.mean(score_basic5) #score melhorado pela cross validation\n",
    "\n",
    "\n",
    "#score_basic5\n",
    "score_cross5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1656  112]\n",
      " [ 581  299]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         NOT       0.74      0.94      0.83      1768\n",
      "         OFF       0.73      0.34      0.46       880\n",
      "\n",
      "    accuracy                           0.74      2648\n",
      "   macro avg       0.73      0.64      0.65      2648\n",
      "weighted avg       0.74      0.74      0.71      2648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Matriz de Confusão para Random Forest\n",
    "pred5 = model5.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred5)) #labels=[\"OFF\",\"NOT\"]\n",
    "print(classification_report(y_test, pred5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = NOT | 1 = OFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.7 - MODELO COM GLOVE & EMPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = np.concatenate(tweets['vec'].to_numpy(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (pd.DataFrame(E)).join(emp_ta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB  --->  precisão: 0.7080815709969789 f1 score: 0.6826949653405112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGR  --->  precisão: 0.7579305135951662 f1 score: 0.7000764224065195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "<ipython-input-105-89430fe00da1>:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf[key].fit(X_train, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM  --->  precisão: 0.7530211480362538 f1 score: 0.6783284742468416\n",
      "RFC  --->  precisão: 0.7466012084592145 f1 score: 0.654165652116114\n",
      "CPU times: user 39.8 s, sys: 364 ms, total: 40.1 s\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classify(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7579305135951662"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model6 = LogisticRegression(random_state=42, max_iter=200)\n",
    "model6.fit(X_train, y_train)\n",
    "#score_basic6 = model6.score(X_test, y_test)\n",
    "score_basic6 = cross_val_score(model6, X, y, cv=5)\n",
    "score_cross6 = np.mean(score_basic6) #score melhorado pela cross validation\n",
    "\n",
    "\n",
    "#score_basic6\n",
    "score_cross6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1585  183]\n",
      " [ 458  422]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.90      0.83      1768\n",
      "           1       0.70      0.48      0.57       880\n",
      "\n",
      "    accuracy                           0.76      2648\n",
      "   macro avg       0.74      0.69      0.70      2648\n",
      "weighted avg       0.75      0.76      0.74      2648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Matriz de Confusão para Logistic Regression\n",
    "pred6 = model6.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred6)) #labels=[\"OFF\",\"NOT\"]\n",
    "print(classification_report(y_test,pred6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = TIM | 1 = UNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8 - MELHOR RESULTADO PARA O SUBTASK A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TF-IDF` com `RANDOM FOREST`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = NOT | 1 = OFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>democrats support musli isis child traffic fun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>constitutionday hate want change</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>foxnews nra maga potus trump veteran iswamp dnc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>watch get news make smile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>unity demo far right london</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>0</td>\n",
       "      <td>lie dem law push kavanaugh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>0</td>\n",
       "      <td>present event 2018 global ampact senior group ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>1</td>\n",
       "      <td>3 people talk sorry y all ass wait i have run ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>1</td>\n",
       "      <td>wednesdaywisdom call right fascist reality lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>0</td>\n",
       "      <td>kavanaugh democrats</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>860 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                              tweet\n",
       "0        1  democrats support musli isis child traffic fun...\n",
       "1        0                   constitutionday hate want change\n",
       "2        0    foxnews nra maga potus trump veteran iswamp dnc\n",
       "3        0                          watch get news make smile\n",
       "4        0                        unity demo far right london\n",
       "..     ...                                                ...\n",
       "855      0                         lie dem law push kavanaugh\n",
       "856      0  present event 2018 global ampact senior group ...\n",
       "857      1  3 people talk sorry y all ass wait i have run ...\n",
       "858      1  wednesdaywisdom call right fascist reality lea...\n",
       "859      0                                kavanaugh democrats\n",
       "\n",
       "[860 rows x 2 columns]"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(tfidf_a.toarray())\n",
    "\n",
    "testset_a_pred = model4.predict(X)   \n",
    "testset_a_pred = pd.DataFrame(testset_a_pred, columns = ['Label'])\n",
    "testset_a_pred = pd.concat([testset_a_pred, testset_a['tweet']],axis=1,sort=False)\n",
    "testset_a_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------\n",
    "# 8 - SUBTASK B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 - DEFININDO O TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = tweets_b['subtask_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarizador_label = LabelBinarizer()\n",
    "t = tweets_b['subtask_b']\n",
    "\n",
    "y = binarizador_label.fit_transform(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 MODELO COM BOW DE FREQUÊNCIA UNIGRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB  --->  precisão: 0.7420454545454546 f1 score: 0.5427148217077687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGR  --->  precisão: 0.8784090909090909 f1 score: 0.4855741126208076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM  --->  precisão: 0.8318181818181818 f1 score: 0.5231679456388026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-105-89430fe00da1>:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf[key].fit(X_train, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC  --->  precisão: 0.865909090909091 f1 score: 0.5298464213271515\n",
      "CPU times: user 1min 44s, sys: 634 ms, total: 1min 45s\n",
      "Wall time: 28.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classify(dfr_bow_tb, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(dfr_bow_tb)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8784090909090909"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model7 = LogisticRegression(random_state=42, max_iter=200)\n",
    "model7.fit(X_train, y_train)\n",
    "#score_basic7 = model7.score(X_test, y_test)\n",
    "score_basic7 = cross_val_score(model7, X, y,cv=5)\n",
    "score_cross7 = np.mean(score_basic7) #score melhorado pela cross validation\n",
    "\n",
    "\n",
    "#score_basic7\n",
    "score_cross7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[771   4]\n",
      " [103   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.99      0.94       775\n",
      "           1       0.33      0.02      0.04       105\n",
      "\n",
      "    accuracy                           0.88       880\n",
      "   macro avg       0.61      0.51      0.49       880\n",
      "weighted avg       0.82      0.88      0.83       880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Matriz de Confusão para Logistic Regression\n",
    "pred7 = model7.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred7)) #labels=[\"OFF\",\"NOT\"]\n",
    "print(classification_report(y_test,pred7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = TIM | 1 = UNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3 - MODELO COM FEATURES DA LINGUAGEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB  --->  precisão: 0.8511363636363637 f1 score: 0.5014509300223586\n",
      "LGR  --->  precisão: 0.8806818181818182 f1 score: 0.46827794561933533\n",
      "SVM  --->  precisão: 0.8806818181818182 f1 score: 0.46827794561933533\n",
      "RFC  --->  precisão: 0.8704545454545455 f1 score: 0.5395715151181363\n",
      "CPU times: user 2.1 s, sys: 169 ms, total: 2.26 s\n",
      "Wall time: 1.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classify(manual_features_tb, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(manual_features_tb)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8806818181818182"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model8 = LogisticRegression(random_state=42, max_iter=200)\n",
    "model8.fit(X_train, y_train)\n",
    "#score_basic8 = model8.score(X_test, y_test)\n",
    "score_basic8 = cross_val_score(model8, X, y,cv=5)\n",
    "score_cross8 = np.mean(score_basic8) #score melhorado pela cross validation\n",
    "\n",
    "\n",
    "#score_basic8\n",
    "score_cross8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[775   0]\n",
      " [105   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94       775\n",
      "           1       0.00      0.00      0.00       105\n",
      "\n",
      "    accuracy                           0.88       880\n",
      "   macro avg       0.44      0.50      0.47       880\n",
      "weighted avg       0.78      0.88      0.82       880\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Matriz de Confusão para Logistic Regression\n",
    "pred8 = model8.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred8)) #labels=[\"OFF\",\"NOT\"]\n",
    "print(classification_report(y_test,pred8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = TIM | 1 = UNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4 - MODELO COM  BOW DE FREQUÊNCIA & FEATURES DA LINGUAGEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfr_bow_tb.join(manual_features_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB  --->  precisão: 0.7420454545454546 f1 score: 0.5427148217077687\n",
      "LGR  --->  precisão: 0.8795454545454545 f1 score: 0.4771769348040535\n",
      "SVM  --->  precisão: 0.8318181818181818 f1 score: 0.5181650018497965\n",
      "RFC  --->  precisão: 0.8806818181818182 f1 score: 0.5035220916549801\n",
      "CPU times: user 1min 9s, sys: 632 ms, total: 1min 10s\n",
      "Wall time: 19.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classify(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-547-501b1ba4d642>:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model9.fit(X_train, y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.884090909090909"
      ]
     },
     "execution_count": 547,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model9 = RandomForestClassifier()\n",
    "model9.fit(X_train, y_train)\n",
    "#score_basic9 = model9.score(X_test, y_test)\n",
    "score_basic9 = cross_val_score(model9, X, y, cv=5)\n",
    "score_cross9 = np.mean(score_basic9) #score melhorado pela cross validation\n",
    "\n",
    "\n",
    "#score_basic9\n",
    "score_cross9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[772   3]\n",
      " [ 99   6]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94       775\n",
      "           1       0.67      0.06      0.11       105\n",
      "\n",
      "    accuracy                           0.88       880\n",
      "   macro avg       0.78      0.53      0.52       880\n",
      "weighted avg       0.86      0.88      0.84       880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Matriz de Confusão para Random Forest\n",
    "pred9 = model9.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred9)) #labels=[\"OFF\",\"NOT\"]\n",
    "print(classification_report(y_test,pred9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = TIM | 1 = UNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.5 - MODELO COM TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB  --->  precisão: 0.740909090909091 f1 score: 0.541917808219178\n",
      "LGR  --->  precisão: 0.8772727272727273 f1 score: 0.46731234866828086\n",
      "SVM  --->  precisão: 0.8454545454545455 f1 score: 0.5044306418219461\n",
      "RFC  --->  precisão: 0.8727272727272727 f1 score: 0.47475005862164527\n",
      "CPU times: user 2min 8s, sys: 755 ms, total: 2min 9s\n",
      "Wall time: 34.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classify(pd.DataFrame(tfidf_tweets_tb.toarray()),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(tfidf_tweets_tb.toarray())\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-440-8f8fb0d3388d>:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  model10.fit(X_train, y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8738636363636364"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model10 = RandomForestClassifier()\n",
    "model10.fit(X_train, y_train)\n",
    "#score_basic10 = model10.score(X_test, y_test)\n",
    "score_basic10 = cross_val_score(model10, X, y, cv=5)\n",
    "score_cross10 = np.mean(score_basic10) #score melhorado pela cross validation\n",
    "\n",
    "\n",
    "#score_basic10\n",
    "score_cross10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[768   7]\n",
      " [104   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.99      0.93       775\n",
      "           1       0.12      0.01      0.02       105\n",
      "\n",
      "    accuracy                           0.87       880\n",
      "   macro avg       0.50      0.50      0.48       880\n",
      "weighted avg       0.79      0.87      0.82       880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Matriz de Confusão para Random Forest\n",
    "pred10 = model10.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred10)) #labels=[\"OFF\",\"NOT\"]\n",
    "print(classification_report(y_test,pred10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = TIM | 1 = UNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.6 - MODELO COM GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "WB = np.concatenate(tweets_b['vec'].to_numpy(), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4400, 300)"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WB.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNB  --->  precisão: 0.740909090909091 f1 score: 0.5666859596230098\n",
      "LGR  --->  precisão: 0.8772727272727273 f1 score: 0.4850901525658807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/svm/_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM  --->  precisão: 0.8738636363636364 f1 score: 0.4918613543080981\n",
      "RFC  --->  precisão: 0.8738636363636364 f1 score: 0.4751519254638361\n",
      "CPU times: user 16.2 s, sys: 228 ms, total: 16.5 s\n",
      "Wall time: 4.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "classify(pd.DataFrame(WB), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(WB)\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/anaconda3/envs/sklearn-env/lib/python3.8/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8772727272727273"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model11 = LogisticRegression(random_state=42, max_iter=200)\n",
    "model11.fit(X_train, y_train)\n",
    "#score_basic11 = model11.score(X_test, y_test)\n",
    "score_basic11 = cross_val_score(model11, X, y, cv=5)\n",
    "score_cross11 = np.mean(score_basic11) #score melhorado pela cross validation\n",
    "\n",
    "\n",
    "#score_basic11\n",
    "score_cross11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[770   5]\n",
      " [103   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.99      0.93       775\n",
      "           1       0.29      0.02      0.04       105\n",
      "\n",
      "    accuracy                           0.88       880\n",
      "   macro avg       0.58      0.51      0.49       880\n",
      "weighted avg       0.81      0.88      0.83       880\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Matriz de Confusão para Logistic Regression\n",
    "pred11 = model11.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred11)) #labels=[\"OFF\",\"NOT\"]\n",
    "print(classification_report(y_test,pred11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = TIM | 1 = UNT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.8 - MELHOR RESULTADO PARA O SUBTASK B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BOW & FEATURES DA LINGUAGEM` com `RANDOM FOREST`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = TIM | 1 = UNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfr_bow_b.join(manual_features_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>democrats support musli child traffic fund ele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>far right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>fuck ti ame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>feel humiliate hi later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nigga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>0</td>\n",
       "      <td>look school point</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>0</td>\n",
       "      <td>ass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0</td>\n",
       "      <td>lie dem law gun control kavanaugh url</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0</td>\n",
       "      <td>3 people talk sorry ass wait i have run funny ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Label                                              tweet\n",
       "0        0  democrats support musli child traffic fund ele...\n",
       "1        0                                          far right\n",
       "2        0                                        fuck ti ame\n",
       "3        0                            feel humiliate hi later\n",
       "4        0                                              nigga\n",
       "..     ...                                                ...\n",
       "235      0                                                   \n",
       "236      0                                  look school point\n",
       "237      0                                                ass\n",
       "238      0              lie dem law gun control kavanaugh url\n",
       "239      0  3 people talk sorry ass wait i have run funny ...\n",
       "\n",
       "[240 rows x 2 columns]"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "testset_b_pred = model9.predict(X)   \n",
    "testset_b_pred = pd.DataFrame(testset_b_pred, columns = ['Label'])\n",
    "testset_b_pred = pd.concat([testset_b_pred, testset_b['tweet']],axis=1,sort=False)\n",
    "testset_b_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
